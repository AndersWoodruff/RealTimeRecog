{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4adade77",
      "metadata": {
        "id": "4adade77"
      },
      "source": [
        "### EMOVOICE: Real-time Speech Emotion Recognition Using Raw Audio Features and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02c13ea",
      "metadata": {
        "id": "b02c13ea"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_wH9-oGOhk5D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wH9-oGOhk5D",
        "outputId": "0420065c-9cd6-4d49-f785-3b53fad0e356"
      },
      "outputs": [],
      "source": [
        "%pip install kagglehub librosa numpy noisereduce evaluate transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f82f34",
      "metadata": {
        "id": "22f82f34"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torchaudio as ta\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch as t\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "from typing import Optional, Callable\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
        "outputId": "49fb8a7c-9020-403d-fc21-3a5d13a9d2b5"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "path = ('/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24')\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YFJ7pTHrxAzU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670,
          "referenced_widgets": [
            "3eea35ad8d594e4e9e2f84b1b803476d",
            "b57d3664717c4d9aa721b8e7f4aaa0a7",
            "6e1dcf93c93148599b719add74e00115",
            "00800023a2994a6fbda5f0d373cba63c",
            "bac8ecc60acb49e3b7e344cb2ae22ad6",
            "e7ae74a1fb2246918d79dad8ff5da065",
            "7cc9f540445a42ba9c70eaba68aea6f6",
            "d5254ea85cfe4af4a75056c3ee918522",
            "d6eba009c9ba4110b451fb287d6891b9",
            "437574cae02e4408bb376938375df6bc",
            "2e7d493184cf4d4ea2f45bbd25f0ca31",
            "6c7f2d2b8c344ab596273e734c672bbb",
            "c7e70b52f674418695d0604ed8f1b780",
            "3f0853a5a23f4aed89ffd7bb491625b5",
            "aa141cc92a7f4c49acf990eb7f944dab",
            "3bb4aaf63824494eb1397e472720ef42",
            "f1c743ba91a549468f8a815942a6dd15",
            "972ad04e703f4459b97b07849b64d762",
            "5baceda9355448ff84e9edc8390362b3",
            "1d3b250ead9b46959e3835e1952050d0",
            "5381a74b4fc744ffa73054dc632ce524",
            "315a658801144a33988286f9808f88a0",
            "f74ae9ea302548be877634784b7a6eed",
            "d701953a8f224ea485e4c2af9375ff9e",
            "da8d43f9b48f4dda8674338811fef36a",
            "38fff488e6b4421e8e5b13ece5cda979",
            "746e81a8d31b49b9b3719b2b91836dff",
            "31583b4f7f4c48a1bc7ec69b49564cd8",
            "3b10f359224c44b7addaa64b5fed962e",
            "bdeece2ef6e6434991e98dd889542356",
            "e6d5601f677249539bd7d1bfe882ebe2",
            "abb8393a3b60477f9991a8127b077ac3",
            "1cd31ba580124a38884c09fb948d6fff"
          ]
        },
        "id": "YFJ7pTHrxAzU",
        "outputId": "b7bc2916-70e2-4f0b-fbd4-27e2e81cedcd"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Wav2Vec2‑base on RAVDESS with AMP, lr = 3e‑5,\n",
        "freeze encoder epoch 0, unfreeze top‑4 layers afterwards.\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path\n",
        "EXP_DIR    = \"./exp_ravdess_amp\"\n",
        "SR         = 16_000\n",
        "SEED       = 40\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "TRAIN_BS   = 4                   # GPU batch\n",
        "EVAL_BS    = 4\n",
        "GRAD_ACCUM = 2                   # 4×2 ⇒ eff 8\n",
        "LR         =  0.0001\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------- DATASET ----------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(name: str) -> int:\n",
        "    return int(EMO_RE.search(name).group(3)) - 1          # 0‑7\n",
        "    # id = int(EMO_RE.search(name).group(3)) - 1\n",
        "    # if id == 0:\n",
        "    #     id = 1\n",
        "    # return id\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "    return wav.clamp_(-0.99, 0.99)\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def double_neutral(self):\n",
        "        paths2 = []\n",
        "        for p in self.paths:\n",
        "            if emotion_id(p.name) == 0:\n",
        "                paths2.append(p)\n",
        "        self.paths = paths2+self.paths\n",
        "        return self\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"wav\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "@dataclass\n",
        "class Collate:\n",
        "    def __call__(self, batch):\n",
        "        audio  = [b[\"wav\"].cpu().numpy() for b in batch]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        proc   = processor(audio, sampling_rate=SR,\n",
        "                           padding=True, return_tensors=\"pt\")\n",
        "        proc[\"labels\"] = labels\n",
        "        return proc\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p).double_neutral(), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p).double_neutral(),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "\n",
        "# ----------------------- MODEL ------------------------\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=8,\n",
        ").to(device)\n",
        "model.gradient_checkpointing_enable()\n",
        "model.freeze_feature_encoder()       # frozen for epoch 0\n",
        "\n",
        "# -------------------- OPT & SCHED ---------------------\n",
        "optimizer = Adam(model.parameters(), lr=LR,\n",
        "                  betas=(0.9,0.999), eps=1e-8)\n",
        "\n",
        "total_steps = (len(train_ld) // GRAD_ACCUM) * NUM_EPOCHS\n",
        "warm_steps  = int(0.1 * total_steps)\n",
        "scheduler   = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warm_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "# ---------------- ENABLE AMP --------------------------\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# -------------- TRAIN / VALIDATE ----------------------\n",
        "def evaluate():\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_ld:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            preds.extend(logits.argmax(-1).cpu())\n",
        "            gts.extend(batch[\"labels\"].cpu())\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "step_global = 0\n",
        "train_loss_history   = []   # step‑level\n",
        "val_acc_history      = []   # epoch‑level\n",
        "best_acc             = 0.0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    # # --------- unfreeze top‑4 layers after epoch 0 ----------\n",
        "    # if epoch == 2:\n",
        "    #     for layer in model.wav2vec2.encoder.layer[-4:]:\n",
        "    #         for p in layer.parameters():\n",
        "    #             p.requires_grad = True\n",
        "    #     print(\"Unfroze top‑4 encoder layers.\")\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step_local, batch in enumerate(pbar, 1):\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = model(**batch).loss / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if step_local % GRAD_ACCUM == 0:\n",
        "            train_loss_history.append(loss.item() * GRAD_ACCUM)\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            step_global += 1\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    acc = evaluate()\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {acc:.4f}\")\n",
        "    val_acc_history.append(acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_model.pt\")\n",
        "\n",
        "print(f\"\\n*** Final accuracy after {NUM_EPOCHS} epochs: {acc:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rf9P24EI2ikw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf9P24EI2ikw",
        "outputId": "a1b216aa-e64e-4828-bf31-8a2a4413efd4"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"./exp_ravdess_amp/best_ckpt\"     # pick any folder name\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)             # ⇦ weights + config\n",
        "processor.save_pretrained(OUTPUT_DIR)         # ⇦ tokenizer / feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EslBPe8boMSc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c4013c06aee74b2c913b59cc55e4a6c7",
            "8520ff0befc4469a9d619466d290483e",
            "d82b6491ad4345318c62e16eca562e1a",
            "6693fcb64b1b4e1691ba5d57ac0341a7",
            "be543b2d94b84d5e999fae0c71fb445a",
            "adf2e2b27b454b17a9d2a917ce946c9c",
            "5cbfd01bfff94ee596a24d02e9cb22ff",
            "15c3b410aebc4ebab2353a705b1e8dd8",
            "70a22b798e7e4a89bb1bfe3c71d3e74d",
            "127ca00050dd4804a5f4ab018f813eb6",
            "eafa8619ba754fac9117a2b44c719421",
            "84e0b8aad7694e8594f1c20b8be15857",
            "aebdc348b01f45dd8ce65518a5e4cd64",
            "f017af5423ac4c95a2bf4c256d5c09d5",
            "3ba64d14d1054b97b238bf4e8e739595",
            "ade30b2cdc1544c28323cf8a98448bdb",
            "704ee3c7aa2b4089854ca8d7c8cb80c6",
            "9271ef0463104054a63629fe30bc3996",
            "36f894dc3b024493a65ebc67085a76f6",
            "5b60f14d899d453ab170399eee81797d",
            "d90ed8f0bac64675a289e73006def398",
            "79a7f9f9697545c2ab88f68388e29ea4",
            "0a3d8883a9ae407e87e67a068f8a3295",
            "6b03cfc8499449138ae677f5e2a97638",
            "a45f6b97fd9a4ea69b74e5c69b81ddf4",
            "bf36f6b52b304ba98961086fc1008bc6",
            "fcec86b78a304430be03df204f0b4a4e",
            "46b8ffdb585f488292a515308043206d",
            "28da138b102a4356aecc35e8dfa787be",
            "73e83e807c1e452da36f423fdf9595e9",
            "cd17dd10103e44539a1c02956a5d5c76",
            "e87363e339fa4305be14470e21753b05",
            "c172b2324ff9496ca3257adba884a2cf"
          ]
        },
        "id": "EslBPe8boMSc",
        "outputId": "e7b779c4-2206-44ad-b7d5-b049ab1ba3ec"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spectrogram CNN‑Transformer for RAVDESS SER\n",
        "-------------------------------------------\n",
        "• Log‑mel spectrogram (128 bins, 25 ms / 10 ms)\n",
        "• 3‑layer 2‑D CNN → Transformer encoder (4 blocks, 8 heads)\n",
        "• Training: AMP, lr 3e‑5, batch 4 (+ grad‑accum 2) for 3 epochs\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path  # <— set me\n",
        "EXP_DIR    = \"./exp_ravdess_spec_tr\"\n",
        "SR         = 16_000\n",
        "N_MELS         = 128\n",
        "NUM_EPOCHS     = 20         # ★\n",
        "LR_MAX         = 5e-5        # ★ peak of 1‑cycle\n",
        "LABEL_SMOOTH   = 0.1        # ★\n",
        "WIN_LEN    = 0.025        # 25 ms\n",
        "HOP_LEN    = 0.010        # 10 ms\n",
        "SEED       = 42\n",
        "TRAIN_BS   = 16\n",
        "EVAL_BS    = 16\n",
        "GRAD_ACCUM = 2\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random, math\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- DATASET -------------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft= int(SR * WIN_LEN),\n",
        "    hop_length= int(SR * HOP_LEN),\n",
        "    n_mels=N_MELS,\n",
        ")\n",
        "db_transform = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "def wav_to_logmel(wav: torch.Tensor) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        spec = mel_spec(wav)        # [1, n_mels, T]\n",
        "        spec_db = db_transform(spec) # log‑mel\n",
        "    return spec_db                  # still [1, n_mels, T]\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True)     # mono, shape [1, L]\n",
        "    wav = wav / (wav.abs().max() + 1e-9)     # normalise\n",
        "    return wav_to_logmel(wav)               # [1, n_mels, T]\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def double_neutral(self):\n",
        "      paths2 = []\n",
        "      for p in self.paths:\n",
        "          if emotion_id(p.name) == 0:\n",
        "              paths2.append(p)\n",
        "      self.paths = paths2+self.paths\n",
        "      return self\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"spec\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in val_p]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "\n",
        "# ------------------ PADDING COLLATE -------------------\n",
        "@dataclass\n",
        "class Collate:\n",
        "    pad_value: float = -80.0\n",
        "    def __call__(self, batch):\n",
        "        specs = [b[\"spec\"].squeeze(0) for b in batch]   # [n_mels, Ti]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        max_T = max(s.shape[1] for s in specs)\n",
        "        padded = torch.stack([\n",
        "            torch.nn.functional.pad(s, (0, max_T - s.shape[1]), value=self.pad_value)\n",
        "            for s in specs\n",
        "        ])                                              # [B, n_mels, max_T]\n",
        "        return {\"spec\": padded.unsqueeze(1), \"labels\": labels}  # add channel dim\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p).double_neutral(), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p).double_neutral(),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "print(val_ld)\n",
        "\n",
        "# --------------- CNN‑TRANSFORMER MODEL ----------------\n",
        "class SpecAug(nn.Module):\n",
        "    def __init__(self, freq_mask=15, time_mask=40, p=0.5):\n",
        "        super().__init__()\n",
        "        self.f = torchaudio.transforms.FrequencyMasking(freq_mask)\n",
        "        self.t = torchaudio.transforms.TimeMasking(time_mask)\n",
        "        self.p = p\n",
        "    def forward(self, x):\n",
        "        if self.training and torch.rand(1) < self.p:\n",
        "            x = self.f(x);  x = self.t(x)\n",
        "        return x\n",
        "\n",
        "spec_aug = SpecAug()\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True) / (wav.abs().max() + 1e-9)\n",
        "    spec = wav_to_logmel(wav)             # [1, mels, T]\n",
        "    return spec_aug(spec)                 # ★ augment on CPU\n",
        "\n",
        "# ---------------- CNN‑Transformer 2.0 -------- ★\n",
        "class SpecCNNTr(nn.Module):\n",
        "    def __init__(self, n_mels=N_MELS, n_classes=8):\n",
        "        super().__init__()\n",
        "        # CNN stem with SE blocks\n",
        "        self.cnn = nn.Sequential(\n",
        "            self._block(1, 64),   # out: [B,64,mel/2,T/2]\n",
        "            self._block(64,128),\n",
        "            self._block(128,192),\n",
        "            self._block(192,256),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "        d_model = 512\n",
        "        self.proj = nn.Linear(256, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=8,\n",
        "            dim_feedforward=1024, dropout=0.3, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.cls = nn.Linear(d_model, n_classes)\n",
        "    def _block(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "    def forward(self, x):                  # x [B,1,mels,T]\n",
        "        x = self.cnn(x)                    # [B,256,F',T']\n",
        "        x = self.pool(x).squeeze(2)        # [B,256,T']\n",
        "        x = self.proj(x.transpose(1,2))    # [B,T',512]\n",
        "        x = self.transformer(x)            # [B,T',512]\n",
        "        return self.cls(x.mean(1))\n",
        "\n",
        "model = SpecCNNTr().to(device)\n",
        "\n",
        "# --------------- optimiser & 1‑cycle ------------ ★\n",
        "optimizer = AdamW(model.parameters(), lr=LR_MAX,\n",
        "                  betas=(0.9,0.98), weight_decay=1e-2, eps=1e-8)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / GRAD_ACCUM)\n",
        "total_steps     = steps_per_epoch * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR_MAX,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1, anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "train_losses, val_accuracies = [], []\n",
        "train_loss_epoch = 0\n",
        "\n",
        "\n",
        "# ------------------- TRAIN / VAL ----------------------\n",
        "def evaluate(loader):\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad(), autocast():\n",
        "        for i, batch in enumerate(loader): # Added 'i' for indexing\n",
        "            spec = batch[\"spec\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device) # Get true labels for comparison\n",
        "\n",
        "            logits = model(spec)\n",
        "            predicted_classes = logits.argmax(-1) # This is a torch.Tensor of predicted class indices\n",
        "\n",
        "            # --- CRITICAL DEBUG FOR EVALUATION ---\n",
        "            if i < 5: # Only print for the first 5 batches in the validation set\n",
        "                print(f\"\\n--- EVALUATION BATCH {i} ---\")\n",
        "                print(f\"  True Labels (Batch): {labels.cpu().numpy()}\")\n",
        "                print(f\"  Predicted Labels (Batch): {predicted_classes.cpu().numpy()}\")\n",
        "\n",
        "                # Check for direct matches\n",
        "                matches = (labels == predicted_classes).cpu().numpy()\n",
        "                print(f\"  Matches: {matches.sum()} / {len(matches)}\")\n",
        "                print(f\"  Accuracy in batch: {matches.mean():.4f}\")\n",
        "\n",
        "                print(f\"  Logits shape: {logits.shape}, Min/Max: {logits.min().item():.4f} / {logits.max().item():.4f}\")\n",
        "                print(\"------------------------------\")\n",
        "            # -------------------------------------\n",
        "\n",
        "            preds.extend(predicted_classes.cpu().tolist()) # Convert to list for extend\n",
        "            gts.extend(labels.cpu().tolist()) # Convert to list for extend\n",
        "    return accuracy_score(gts, preds)\n",
        "scaler = GradScaler()\n",
        "best_acc = 0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, batch in enumerate(pbar, 1):\n",
        "        spec = batch[\"spec\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            logits = model(spec)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels) / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        train_loss_epoch += loss.item() * GRAD_ACCUM\n",
        "        if step % GRAD_ACCUM == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            train_losses.append(train_loss_epoch / steps_per_epoch)\n",
        "            train_loss_epoch = 0\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    val_acc = evaluate(val_ld)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_cnn.pt\")\n",
        "\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n*** Training done. Final val acc: {val_acc:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uKzIkJHBxK9y",
      "metadata": {
        "id": "uKzIkJHBxK9y"
      },
      "outputs": [],
      "source": [
        "import json, numpy as np, pathlib, datetime as dt\n",
        "stats = {\"loss\": train_losses, \"val_acc\": val_accuracies}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KCpvs_zHxLbg",
      "metadata": {
        "id": "KCpvs_zHxLbg"
      },
      "outputs": [],
      "source": [
        "import json, matplotlib.pyplot as plt, numpy as np, pathlib\n",
        "\n",
        "epochs = np.arange(1, len(train_losses) + 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = np.array(train_losses)\n",
        "window = 50\n",
        "\n",
        "# Compute the moving average (valid mode so we don’t pad)\n",
        "smoothed = np.convolve(loss, np.ones(window)/window, mode=\"valid\")\n",
        "\n",
        "# The x‐axis for the smoothed curve should start at “window/2” roughly\n",
        "iters = np.arange(len(smoothed)) + window//2\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iters, smoothed, label=f\"loss (smoothed, window={window})\")\n",
        "plt.xlabel(\"Training iteration\")\n",
        "plt.ylabel(\"Cross‐entropy loss\")\n",
        "plt.title(\"Smoothed training‐loss – Spectrogram CNN-Transformer\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_curve_smoothed.png\")\n",
        "plt.show()\n",
        "\n",
        "print(train_losses[-1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-KodporP1Gme",
      "metadata": {
        "id": "-KodporP1Gme"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "ckpt_dir = OUTPUT_DIR   # your HF dir\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(ckpt_dir)\n",
        "model.eval()\n",
        "\n",
        "# Save weights as a plain .pt state‑dict\n",
        "torch.save(model.state_dict(), \"best_w2v2.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ohCTqrcI3kwk",
      "metadata": {
        "id": "ohCTqrcI3kwk"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# fusion_ravdess.py\n",
        "\n",
        "\"\"\"\n",
        "Late-fusion ensemble for RAVDESS:\n",
        "    • Branch A: Wav2Vec2-base classifier\n",
        "    • Branch B: Spectrogram CNN-Transformer classifier\n",
        "    • LogisticRegression over softmax outputs (16-dim feature)\n",
        "    • Stratified validation split, softmax inputs, C=500 regularization\n",
        "    • MelSpectrogram parameters exactly match SpecCNNTr training:\n",
        "        – SR = 16 kHz, n_fft = int(16 000 × 0.025) = 400\n",
        "        – hop_length = int(16 000 × 0.010) = 160\n",
        "        – n_mels = 128\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "# ─── PATHS & CONFIGURATION ───────────────────────────────────────────────\n",
        "DATA_DIR   = path  # adjust to your environment\n",
        "W2V2_PT    = \"best_model.pt\"         # your fine-tuned Wav2Vec2 weights\n",
        "SPEC_PT    = \"best_cnn.pt\"         # your fine-tuned SpecCNNTr weights\n",
        "FUSION_OUT = \"./fusion_head2.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── LABEL UTILITY ───────────────────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in val_paths]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "print(f\"Training on {len(train_paths)} clips, validating on {len(val_paths)} clips.\")\n",
        "\n",
        "# ─── BRANCH A: Wav2Vec2-base ───────────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "\n",
        "state_w2v = torch.load(W2V2_PT, map_location=device)\n",
        "w2v2_model.load_state_dict(state_w2v)\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor @16 kHz, normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    inp = processor(wav.numpy(),\n",
        "                    sampling_rate=SR,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)  # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()         # [8]\n",
        "\n",
        "# ─── BRANCH B: Spectrogram CNN-Transformer ──────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "state_spec = torch.load(SPEC_PT, map_location=device)\n",
        "spec_model.load_state_dict(state_spec)\n",
        "spec_model.eval()\n",
        "\n",
        "# MelSpectrogram parameters exactly as used during SpecCNNTr training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 16 000 × 0.025 = 400\n",
        "    hop_length=int(SR * 0.010), # 16 000 × 0.010 = 160\n",
        "    n_mels=128                  # matches N_MELS during training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor [L], normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    # 1) Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1, 128, T']\n",
        "    spec_db = to_db(spec)               # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1, 1, 128, T']\n",
        "\n",
        "    # 2) Forward through the trained SpecCNNTr\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()      # [8]\n",
        "\n",
        "# ─── STEP 1: VERIFY EACH BRANCH ON VALIDATION SET ─────────────────────────\n",
        "def evaluate_branch(predict_fn, paths):\n",
        "    preds, gts = [], []\n",
        "    for p in paths:\n",
        "        wav, sr = torchaudio.load(p)\n",
        "        if sr != SR:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "        wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "        prob = predict_fn(wav)           # [8]\n",
        "        preds.append(int(prob.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "acc_w2v2 = evaluate_branch(probs_w2v2, val_paths)\n",
        "acc_spec = evaluate_branch(probs_spec, val_paths)\n",
        "print(f\"Wav2Vec2 branch accuracy on val:   {acc_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch accuracy on val:   {acc_spec:.4f}\")\n",
        "\n",
        "# ─── STEP 2: TRAIN LOGISTIC-REGRESSION FUSION ON SOFTMAX INPUTS ──────────\n",
        "X, y = [], []\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "    p1 = probs_w2v2(wav)           # [8]\n",
        "    p2 = probs_spec(wav)           # [8]\n",
        "    X.append(torch.cat([p1, p2]).numpy())  # 16-dim feature\n",
        "    y.append(emotion_id(str(p)))\n",
        "\n",
        "fusion = LogisticRegression(\n",
        "    C=500,                    # low L2 penalty → near-exact fit if possible\n",
        "    max_iter=2000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\"\n",
        ")\n",
        "fusion.fit(X, y)\n",
        "train_acc_lr = accuracy_score(y, fusion.predict(X))\n",
        "print(f\"LogisticRegression (train-on-val) acc = {train_acc_lr:.4f}\")\n",
        "\n",
        "with open(FUSION_OUT, \"wb\") as fout:\n",
        "    pickle.dump(fusion, fout)\n",
        "print(f\"Saved fusion head → {FUSION_OUT}\")\n",
        "\n",
        "# ─── STEP 3: SANITY CHECK ON A RANDOM VALIDATION CLIP ─────────────────────\n",
        "import numpy as np\n",
        "test_clip = random.choice(val_paths)\n",
        "wav, sr    = torchaudio.load(test_clip)\n",
        "if sr != SR:\n",
        "    wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "p1 = probs_w2v2(wav).numpy()\n",
        "p2 = probs_spec(wav).numpy()\n",
        "fused_probs = fusion.predict_proba([np.concatenate([p1, p2])])[0]\n",
        "predicted = int(fused_probs.argmax())\n",
        "true_label = emotion_id(str(test_clip))\n",
        "\n",
        "print(f\"Sanity check clip: {test_clip.name}\")\n",
        "print(f\" W2V2 predicts {int(p1.argmax())}, SpecCNNTr predicts {int(p2.argmax())}\")\n",
        "print(f\" Fusion predicts {predicted}, true = {true_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_ghUeP4zSRp",
      "metadata": {
        "id": "E_ghUeP4zSRp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# ─── IMPORT YOUR SPEC MODEL CLASS ─────────────────────────────────────────\n",
        "\n",
        "\n",
        "# ─── CONFIGURATION ────────────────────────────────────────────────────────\n",
        "DATA_DIR   = path # adjust\n",
        "folder     = r\"\"\n",
        "W2V2_PT    = folder + \"best_w2v2.pt\"\n",
        "SPEC_PT    = folder + \"best_cnn.pt\"\n",
        "FUSION_PT  = folder + \"fusion_head2.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "ALPHA      = 0.5  # for weighted average\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── UTILITY: EXTRACT EMOTION ID ───────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "_, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# ─── LOAD BRANCH A: Wav2Vec2-base ─────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2_model.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD BRANCH B: SpecCNNTr ─────────────────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "# Use the same MelSpectrogram settings from training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 0.025 s at 16 kHz → n_fft = 400\n",
        "    hop_length=int(SR * 0.010), # 0.010 s at 16 kHz → hop_length = 160\n",
        "    n_mels=128                  # exactly as in SpecCNNTr’s training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1,128,T']\n",
        "    spec_db = to_db(spec)               # [1,128,T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1,1,128,T']\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD FUSION HEAD (Logistic Regression) ──────────────────────────────\n",
        "fusion = pickle.load(open(FUSION_PT, \"rb\"))\n",
        "\n",
        "# ─── EVALUATE ALL FOUR MODELS ─────────────────────────────────────────────\n",
        "y_true, preds_w2v2, preds_spec, preds_avg, preds_lr = [], [], [], [], []\n",
        "\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # normalize\n",
        "\n",
        "    # ground truth\n",
        "    true = emotion_id(str(p))\n",
        "    y_true.append(true)\n",
        "\n",
        "    # branch A: Wav2Vec2\n",
        "    p1 = probs_w2v2(wav).numpy()    # [8]\n",
        "    pred1 = int(np.argmax(p1))\n",
        "    preds_w2v2.append(pred1)\n",
        "\n",
        "    # branch B: SpecCNNTr\n",
        "    p2 = probs_spec(wav).numpy()    # [8]\n",
        "    pred2 = int(np.argmax(p2))\n",
        "    preds_spec.append(pred2)\n",
        "\n",
        "    # weighted average (alpha = 0.5)\n",
        "    fused_probs = ALPHA * p1 + (1 - ALPHA) * p2\n",
        "    preds_avg.append(int(np.argmax(fused_probs)))\n",
        "\n",
        "    # logistic regression fusion\n",
        "    fused_lr = fusion.predict([np.concatenate([p1, p2])])[0]\n",
        "    preds_lr.append(int(fused_lr))\n",
        "\n",
        "# ─── COMPUTE F1 SCORES (MACRO) ─────────────────────────────────────────────\n",
        "f1_w2v2 = f1_score(y_true, preds_w2v2, average=\"macro\")\n",
        "f1_spec = f1_score(y_true, preds_spec, average=\"macro\")\n",
        "f1_avg  = f1_score(y_true, preds_avg,  average=\"macro\")\n",
        "f1_lr   = f1_score(y_true, preds_lr,   average=\"macro\")\n",
        "# ─── VALIDATION LOSS CALCULATION ─────────────────────────────────────────────\n",
        "\n",
        "# We’ll accumulate total loss and then divide by number of samples:\n",
        "total_w2v2_loss = 0.0\n",
        "total_spec_loss = 0.0\n",
        "total_avg_loss  = 0.0\n",
        "total_lr_loss   = 0.0\n",
        "n = len(val_paths)\n",
        "\n",
        "for p in val_paths:\n",
        "    # Load & normalize waveform\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "\n",
        "    true_label = emotion_id(str(p))\n",
        "    label_tensor = torch.tensor([true_label]).long()  # shape [1]\n",
        "\n",
        "    # ---- Wav2Vec2 branch loss ----\n",
        "    # Get raw logits from Wav2Vec2 (before softmax)\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits_w2v = w2v2_model(**inp).logits.squeeze(0)  # [8] on device\n",
        "    loss_w2v2 = F.cross_entropy(logits_w2v.unsqueeze(0), label_tensor.to(device))\n",
        "    total_w2v2_loss += loss_w2v2.item()\n",
        "\n",
        "    # ---- SpecCNNTr branch loss ----\n",
        "    # Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))        # [1, 128, T']\n",
        "    spec_db = to_db(spec)                    # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)   # [1, 1, 128, T']\n",
        "    logits_spec = spec_model(spec_db).squeeze(0)  # [8] on device\n",
        "    loss_spec = F.cross_entropy(logits_spec.unsqueeze(0), label_tensor.to(device))\n",
        "    total_spec_loss += loss_spec.item()\n",
        "\n",
        "    # ---- Weighted-average ensemble loss ----\n",
        "    # We already have p1, p2 as probabilities, but we need fused_probs\n",
        "    with torch.no_grad():\n",
        "        # Recompute probabilities on CPU\n",
        "        p1 = F.softmax(logits_w2v, dim=-1).cpu().numpy()   # [8]\n",
        "        p2 = F.softmax(logits_spec, dim=-1).cpu().numpy()  # [8]\n",
        "        fused_probs = ALPHA * p1 + (1 - ALPHA) * p2         # [8] on CPU\n",
        "\n",
        "    # Cross-entropy for weighted average: -log(fused_probs[true_label])\n",
        "    eps = 1e-9  # avoid log(0)\n",
        "    total_avg_loss += -np.log(fused_probs[true_label] + eps)\n",
        "\n",
        "    # ---- Logistic Regression fusion loss ----\n",
        "    # Use fusion.predict_proba to get probabilities, then compute -log for true class\n",
        "    lr_probs = fusion.predict_proba([np.concatenate([p1, p2])])  # [8]\n",
        "    # print(lr_probs)\n",
        "    # total_lr_loss += -np.log(lr_probs[true_label] + eps)\n",
        "\n",
        "# Compute average loss over validation set\n",
        "val_loss_w2v2 = total_w2v2_loss / n\n",
        "val_loss_spec = total_spec_loss / n\n",
        "val_loss_avg  = total_avg_loss  / n\n",
        "# val_loss_lr   = total_lr_loss   / n\n",
        "\n",
        "# ─── PRINT VALIDATION LOSSES ─────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch   –  Val Loss = {val_loss_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch  –  Val Loss = {val_loss_spec:.4f}\")\n",
        "print(f\"Weighted Avg       –  Val Loss = {val_loss_avg:.4f}\")\n",
        "# print(f\"LR Fusion          –  Val Loss = {val_loss_lr:.4f}\")\n",
        "# ─── PRINT RESULTS ─────────────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch  –  Val Accuracy = {accuracy_score(y_true, preds_w2v2):.4f},  F1 (macro) = {f1_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch –  Val Accuracy = {accuracy_score(y_true, preds_spec):.4f},  F1 (macro) = {f1_spec:.4f}\")\n",
        "print(f\"Weighted Avg      –  Val Accuracy = {accuracy_score(y_true, preds_avg):.4f},  F1 (macro) = {f1_avg:.4f}\")\n",
        "print(f\"LR Fusion         –  Val Accuracy = {accuracy_score(y_true, preds_lr):.4f},  F1 (macro) = {f1_lr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8QMwZzs84ozg",
      "metadata": {
        "id": "8QMwZzs84ozg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Import your spectrogram model class and the wav→logmel helper\n",
        "\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────\n",
        "DATA_DIR      = path   # adjust\n",
        "W2V2_PT       = \"best_w2v2.pt\"\n",
        "SPEC_PT       = \"best_cnn.pt\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ── LABEL EXTRACTION ───────────────────────────────────────────\n",
        "import re\n",
        "EMO_RE = re.compile(r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "                    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1  # 0–7\n",
        "\n",
        "# ── COLLECT & SPLIT ───────────────────────────────────────────\n",
        "all_wavs = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_wavs,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=[emotion_id(str(p)) for p in all_wavs],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# ── LOAD Wav2Vec2 BRANCH ───────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_w2v2(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(\n",
        "        wav_tensor.numpy(),\n",
        "        sampling_rate=SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "    logits = w2v2(**inp).logits  # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)  # [8]\n",
        "\n",
        "# ── LOAD SPECTROGRAM BRANCH ───────────────────────────────────\n",
        "\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_spec(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    spec = wav_to_logmel(wav_tensor.unsqueeze(0))     # [1, n_mels, T]\n",
        "    spec = spec.unsqueeze(0).to(device)                # [1, 1, n_mels, T]\n",
        "    logits = spec_model(spec)                          # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)\n",
        "\n",
        "# ── EVALUATION UTILS ─────────────────────────────────────────\n",
        "def load_and_normalize(path: Path) -> torch.Tensor:\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "    return wav\n",
        "\n",
        "def evaluate_ensemble(alpha: float, val_files):\n",
        "    preds, gts = [], []\n",
        "    for p in val_files:\n",
        "        wav = load_and_normalize(p)\n",
        "        p1 = logits_w2v2(wav)   # [8]\n",
        "        p2 = logits_spec(wav)   # [8]\n",
        "        fused = alpha * p1 + (1.0 - alpha) * p2\n",
        "        preds.append(int(fused.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "# ── SEARCH BEST ALPHA ─────────────────────────────────────────\n",
        "best_alpha, best_acc = 0.0, 0.0\n",
        "for alpha in np.linspace(0.0, 1.0, 3):  # 0.0, 0.05, ..., 1.0\n",
        "    acc = evaluate_ensemble(alpha, val_paths)\n",
        "    if acc > best_acc:\n",
        "        best_acc, best_alpha = acc, alpha\n",
        "\n",
        "print(f\"Best weighted‐avg α = {best_alpha:.2f}, Val Acc = {best_acc:.4f}\")\n",
        "\n",
        "# ── OPTIONALLY: PRINT A FEW SAMPLE PREDICTIONS ─────────────────\n",
        "for i, p in enumerate(random.sample(val_paths, 5), 1):\n",
        "    wav = load_and_normalize(p)\n",
        "    p1 = logits_w2v2(wav).argmax().item()\n",
        "    p2 = logits_spec(wav).argmax().item()\n",
        "    fused = (best_alpha * logits_w2v2(wav) + (1.0 - best_alpha) * logits_spec(wav)).argmax().item()\n",
        "    true = emotion_id(str(p))\n",
        "    print(f\"{i}) {p.name} → W2V2={p1}, SPEC={p2}, FUSED={fused}, TRUE={true}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cnZU7WWBsoCH",
      "metadata": {
        "id": "cnZU7WWBsoCH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00800023a2994a6fbda5f0d373cba63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_437574cae02e4408bb376938375df6bc",
            "placeholder": "​",
            "style": "IPY_MODEL_2e7d493184cf4d4ea2f45bbd25f0ca31",
            "value": "Epoch 1/3: 100% 308/308 [01:48&lt;00:00,  3.75it/s, loss=1.3418, lr=7.4e-5]"
          }
        },
        "0a3d8883a9ae407e87e67a068f8a3295": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b03cfc8499449138ae677f5e2a97638",
              "IPY_MODEL_a45f6b97fd9a4ea69b74e5c69b81ddf4",
              "IPY_MODEL_bf36f6b52b304ba98961086fc1008bc6"
            ],
            "layout": "IPY_MODEL_fcec86b78a304430be03df204f0b4a4e"
          }
        },
        "127ca00050dd4804a5f4ab018f813eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c3b410aebc4ebab2353a705b1e8dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd31ba580124a38884c09fb948d6fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d3b250ead9b46959e3835e1952050d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28da138b102a4356aecc35e8dfa787be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e7d493184cf4d4ea2f45bbd25f0ca31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31583b4f7f4c48a1bc7ec69b49564cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315a658801144a33988286f9808f88a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36f894dc3b024493a65ebc67085a76f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38fff488e6b4421e8e5b13ece5cda979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb8393a3b60477f9991a8127b077ac3",
            "placeholder": "​",
            "style": "IPY_MODEL_1cd31ba580124a38884c09fb948d6fff",
            "value": "Epoch 3/3: 100% 308/308 [01:12&lt;00:00,  4.38it/s, loss=1.9219, lr=0]"
          }
        },
        "3b10f359224c44b7addaa64b5fed962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ba64d14d1054b97b238bf4e8e739595": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90ed8f0bac64675a289e73006def398",
            "placeholder": "​",
            "style": "IPY_MODEL_79a7f9f9697545c2ab88f68388e29ea4",
            "value": "Epoch 2/20: 100% 77/77 [00:40&lt;00:00,  2.61it/s, loss=1.4332, lr=5e-5]"
          }
        },
        "3bb4aaf63824494eb1397e472720ef42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "3eea35ad8d594e4e9e2f84b1b803476d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b57d3664717c4d9aa721b8e7f4aaa0a7",
              "IPY_MODEL_6e1dcf93c93148599b719add74e00115",
              "IPY_MODEL_00800023a2994a6fbda5f0d373cba63c"
            ],
            "layout": "IPY_MODEL_bac8ecc60acb49e3b7e344cb2ae22ad6"
          }
        },
        "3f0853a5a23f4aed89ffd7bb491625b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5baceda9355448ff84e9edc8390362b3",
            "max": 308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d3b250ead9b46959e3835e1952050d0",
            "value": 308
          }
        },
        "437574cae02e4408bb376938375df6bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b8ffdb585f488292a515308043206d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5381a74b4fc744ffa73054dc632ce524": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b60f14d899d453ab170399eee81797d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5baceda9355448ff84e9edc8390362b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cbfd01bfff94ee596a24d02e9cb22ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6693fcb64b1b4e1691ba5d57ac0341a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127ca00050dd4804a5f4ab018f813eb6",
            "placeholder": "​",
            "style": "IPY_MODEL_eafa8619ba754fac9117a2b44c719421",
            "value": "Epoch 1/20: 100% 77/77 [00:33&lt;00:00,  2.61it/s, loss=1.8097, lr=2.55e-5]"
          }
        },
        "6b03cfc8499449138ae677f5e2a97638": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46b8ffdb585f488292a515308043206d",
            "placeholder": "​",
            "style": "IPY_MODEL_28da138b102a4356aecc35e8dfa787be",
            "value": ""
          }
        },
        "6c7f2d2b8c344ab596273e734c672bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7e70b52f674418695d0604ed8f1b780",
              "IPY_MODEL_3f0853a5a23f4aed89ffd7bb491625b5",
              "IPY_MODEL_aa141cc92a7f4c49acf990eb7f944dab"
            ],
            "layout": "IPY_MODEL_3bb4aaf63824494eb1397e472720ef42"
          }
        },
        "6e1dcf93c93148599b719add74e00115": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5254ea85cfe4af4a75056c3ee918522",
            "max": 308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6eba009c9ba4110b451fb287d6891b9",
            "value": 308
          }
        },
        "704ee3c7aa2b4089854ca8d7c8cb80c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70a22b798e7e4a89bb1bfe3c71d3e74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73e83e807c1e452da36f423fdf9595e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746e81a8d31b49b9b3719b2b91836dff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "79a7f9f9697545c2ab88f68388e29ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cc9f540445a42ba9c70eaba68aea6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84e0b8aad7694e8594f1c20b8be15857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aebdc348b01f45dd8ce65518a5e4cd64",
              "IPY_MODEL_f017af5423ac4c95a2bf4c256d5c09d5",
              "IPY_MODEL_3ba64d14d1054b97b238bf4e8e739595"
            ],
            "layout": "IPY_MODEL_ade30b2cdc1544c28323cf8a98448bdb"
          }
        },
        "8520ff0befc4469a9d619466d290483e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf2e2b27b454b17a9d2a917ce946c9c",
            "placeholder": "​",
            "style": "IPY_MODEL_5cbfd01bfff94ee596a24d02e9cb22ff",
            "value": ""
          }
        },
        "9271ef0463104054a63629fe30bc3996": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "972ad04e703f4459b97b07849b64d762": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a45f6b97fd9a4ea69b74e5c69b81ddf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73e83e807c1e452da36f423fdf9595e9",
            "max": 77,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd17dd10103e44539a1c02956a5d5c76",
            "value": 21
          }
        },
        "aa141cc92a7f4c49acf990eb7f944dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5381a74b4fc744ffa73054dc632ce524",
            "placeholder": "​",
            "style": "IPY_MODEL_315a658801144a33988286f9808f88a0",
            "value": "Epoch 2/3: 100% 308/308 [01:15&lt;00:00,  4.68it/s, loss=2.1738, lr=3.7e-5]"
          }
        },
        "abb8393a3b60477f9991a8127b077ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade30b2cdc1544c28323cf8a98448bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "adf2e2b27b454b17a9d2a917ce946c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aebdc348b01f45dd8ce65518a5e4cd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704ee3c7aa2b4089854ca8d7c8cb80c6",
            "placeholder": "​",
            "style": "IPY_MODEL_9271ef0463104054a63629fe30bc3996",
            "value": ""
          }
        },
        "b57d3664717c4d9aa721b8e7f4aaa0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ae74a1fb2246918d79dad8ff5da065",
            "placeholder": "​",
            "style": "IPY_MODEL_7cc9f540445a42ba9c70eaba68aea6f6",
            "value": ""
          }
        },
        "bac8ecc60acb49e3b7e344cb2ae22ad6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "bdeece2ef6e6434991e98dd889542356": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be543b2d94b84d5e999fae0c71fb445a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "bf36f6b52b304ba98961086fc1008bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87363e339fa4305be14470e21753b05",
            "placeholder": "​",
            "style": "IPY_MODEL_c172b2324ff9496ca3257adba884a2cf",
            "value": "Epoch 3/20:  27% 21/77 [00:08&lt;00:24,  2.29it/s, loss=1.7101, lr=5e-5]"
          }
        },
        "c172b2324ff9496ca3257adba884a2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4013c06aee74b2c913b59cc55e4a6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8520ff0befc4469a9d619466d290483e",
              "IPY_MODEL_d82b6491ad4345318c62e16eca562e1a",
              "IPY_MODEL_6693fcb64b1b4e1691ba5d57ac0341a7"
            ],
            "layout": "IPY_MODEL_be543b2d94b84d5e999fae0c71fb445a"
          }
        },
        "c7e70b52f674418695d0604ed8f1b780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c743ba91a549468f8a815942a6dd15",
            "placeholder": "​",
            "style": "IPY_MODEL_972ad04e703f4459b97b07849b64d762",
            "value": ""
          }
        },
        "cd17dd10103e44539a1c02956a5d5c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5254ea85cfe4af4a75056c3ee918522": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6eba009c9ba4110b451fb287d6891b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d701953a8f224ea485e4c2af9375ff9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31583b4f7f4c48a1bc7ec69b49564cd8",
            "placeholder": "​",
            "style": "IPY_MODEL_3b10f359224c44b7addaa64b5fed962e",
            "value": ""
          }
        },
        "d82b6491ad4345318c62e16eca562e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15c3b410aebc4ebab2353a705b1e8dd8",
            "max": 77,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70a22b798e7e4a89bb1bfe3c71d3e74d",
            "value": 77
          }
        },
        "d90ed8f0bac64675a289e73006def398": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da8d43f9b48f4dda8674338811fef36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdeece2ef6e6434991e98dd889542356",
            "max": 308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6d5601f677249539bd7d1bfe882ebe2",
            "value": 308
          }
        },
        "e6d5601f677249539bd7d1bfe882ebe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7ae74a1fb2246918d79dad8ff5da065": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e87363e339fa4305be14470e21753b05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafa8619ba754fac9117a2b44c719421": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f017af5423ac4c95a2bf4c256d5c09d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36f894dc3b024493a65ebc67085a76f6",
            "max": 77,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b60f14d899d453ab170399eee81797d",
            "value": 77
          }
        },
        "f1c743ba91a549468f8a815942a6dd15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74ae9ea302548be877634784b7a6eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d701953a8f224ea485e4c2af9375ff9e",
              "IPY_MODEL_da8d43f9b48f4dda8674338811fef36a",
              "IPY_MODEL_38fff488e6b4421e8e5b13ece5cda979"
            ],
            "layout": "IPY_MODEL_746e81a8d31b49b9b3719b2b91836dff"
          }
        },
        "fcec86b78a304430be03df204f0b4a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
