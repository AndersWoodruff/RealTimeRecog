{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4adade77",
      "metadata": {
        "id": "4adade77"
      },
      "source": [
        "### EMOVOICE: Real-time Speech Emotion Recognition Using Raw Audio Features and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02c13ea",
      "metadata": {
        "id": "b02c13ea"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "_wH9-oGOhk5D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wH9-oGOhk5D",
        "outputId": "66c639ba-a93e-4fbf-b719-cd7fd4dbe6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.3.12)\n",
            "Requirement already satisfied: librosa in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.26.4)\n",
            "Requirement already satisfied: noisereduce in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.0.3)\n",
            "Requirement already satisfied: evaluate in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[torch] in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.51.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (0.61.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (5.2.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from noisereduce) (3.10.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: dill in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: xxhash in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from evaluate) (0.31.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers[torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: torch>=2.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers[torch]) (2.6.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers[torch]) (1.6.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->kagglehub) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: networkx in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ander\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->evaluate) (2025.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub librosa numpy noisereduce evaluate transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22f82f34",
      "metadata": {
        "id": "22f82f34"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torchaudio as ta\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch as t\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "from typing import Optional, Callable\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
        "outputId": "0f38aa2a-5abe-4599-c51c-1205705800f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "path = ('/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24')\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YFJ7pTHrxAzU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686,
          "referenced_widgets": [
            "71fa3b0f11bf4785ae1a3326f07783e4",
            "ca50560a4ec04e0ca45fffa23cf22ddb",
            "78346f06fb5544c785789517f5521d1e",
            "f6dbb4c8e0cd4aaca81be647b79b01b5",
            "6773e6823d244a1fa1fceea6aeeb5b20",
            "ce6124a311024052ab90a2d9181cb075",
            "25f74e6963384a59b117508a57930e8e",
            "2f6417545f124023be3fb3af07612d2d",
            "6fd0893b6cb748c0b814f11b6d9d9cd9",
            "82ec52e11c6a4d629af2946aa3b0b444",
            "4501d091a54941d79c574a88fed80359",
            "cac18347c55a4981b5ab1fb5fdae3287",
            "424395352d6548f9938c8dd1322bcd0f",
            "30f1d6f5b7364d2381a39912a82ecd36",
            "cb2a7a7fc56f4a8186c90c3d18e1b76b",
            "03eaba4bd20a4b58bd0021c812e6d6de",
            "4a9b96f5274b4f6b806f6172d2c1109a",
            "2568e99ff5284c0ca5462883f217373a",
            "f321fa64bc144b4c931a70ea83babb94",
            "65447255395e467c8a8825d6be011812",
            "640d4bb42cdf4b4ab1fb649c435f6a3b",
            "89805eab08e54e878dfaa806215b48e9",
            "d7d4e3a314514c1daf68d9a1e4939d70",
            "388151add624408ca0f4d21fe59db0a8",
            "d4019cbb45464a8abba16fd10f0f177d",
            "a2789dae1eb24dbf900ad967ddcd5407",
            "d98d219351a6415a8663fcfba8142db2",
            "e2db9c68aee54f3ebbcd87483d9dda2c",
            "75b7bf0245fb4b0d972cd60f08e73016",
            "b4a0e4fa7edb463fad4e63246f0e3254",
            "d8a4d47d5d354566b3ccb8e8623b6e5f",
            "105188ef7f194f99b19fafb62e9f033d",
            "3cf0eb6b1b4a409181b435eaddfb280e"
          ]
        },
        "id": "YFJ7pTHrxAzU",
        "outputId": "1c2572ce-6cfc-4cf0-b917-5a253ebac41e"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m: load_audio(p), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: emotion_id(p\u001b[38;5;241m.\u001b[39mname)}\n\u001b[0;32m     64\u001b[0m all_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(Path(DATA_DIR)\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m train_p, val_p \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43memotion_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCollate\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2478\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2485\u001b[0m     )\n\u001b[0;32m   2487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
            "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Wav2Vec2‑base on RAVDESS with AMP, lr = 3e‑5,\n",
        "freeze encoder epoch 0, unfreeze top‑4 layers afterwards.\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path\n",
        "EXP_DIR    = \"./exp_ravdess_amp\"\n",
        "SR         = 16_000\n",
        "SEED       = 40\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "TRAIN_BS   = 4                   # GPU batch\n",
        "EVAL_BS    = 4\n",
        "GRAD_ACCUM = 2                   # 4×2 ⇒ eff 8\n",
        "LR         =  0.0001\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------- DATASET ----------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(name: str) -> int:\n",
        "    # return int(EMO_RE.search(name).group(3)) - 1          # 0‑7\n",
        "    id = int(EMO_RE.search(name).group(3)) - 1\n",
        "    if id == 0:\n",
        "        id = 1\n",
        "    return id\n",
        "\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "    return wav.clamp_(-0.99, 0.99)\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def double_neutral(self):\n",
        "        for p in self.paths:\n",
        "            if emotion_id(p.name) == 0:\n",
        "                self.paths.append(p)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"wav\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "@dataclass\n",
        "class Collate:\n",
        "    def __call__(self, batch):\n",
        "        audio  = [b[\"wav\"].cpu().numpy() for b in batch]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        proc   = processor(audio, sampling_rate=SR,\n",
        "                           padding=True, return_tensors=\"pt\")\n",
        "        proc[\"labels\"] = labels\n",
        "        return proc\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "\n",
        "# ----------------------- MODEL ------------------------\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=8,\n",
        ").to(device)\n",
        "model.gradient_checkpointing_enable()\n",
        "model.freeze_feature_encoder()       # frozen for epoch 0\n",
        "\n",
        "# -------------------- OPT & SCHED ---------------------\n",
        "optimizer = Adam(model.parameters(), lr=LR,\n",
        "                  betas=(0.9,0.999), eps=1e-8)\n",
        "\n",
        "total_steps = (len(train_ld) // GRAD_ACCUM) * NUM_EPOCHS\n",
        "warm_steps  = int(0.1 * total_steps)\n",
        "scheduler   = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warm_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "# ---------------- ENABLE AMP --------------------------\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# -------------- TRAIN / VALIDATE ----------------------\n",
        "def evaluate():\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_ld:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            preds.extend(logits.argmax(-1).cpu())\n",
        "            gts.extend(batch[\"labels\"].cpu())\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "step_global = 0\n",
        "train_loss_history   = []   # step‑level\n",
        "val_acc_history      = []   # epoch‑level\n",
        "best_acc             = 0.0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    # # --------- unfreeze top‑4 layers after epoch 0 ----------\n",
        "    # if epoch == 2:\n",
        "    #     for layer in model.wav2vec2.encoder.layer[-4:]:\n",
        "    #         for p in layer.parameters():\n",
        "    #             p.requires_grad = True\n",
        "    #     print(\"Unfroze top‑4 encoder layers.\")\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step_local, batch in enumerate(pbar, 1):\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = model(**batch).loss / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if step_local % GRAD_ACCUM == 0:\n",
        "            train_loss_history.append(loss.item() * GRAD_ACCUM)\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            step_global += 1\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    acc = evaluate()\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {acc:.4f}\")\n",
        "    val_acc_history.append(acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_model.pt\")\n",
        "\n",
        "print(f\"\\n*** Final accuracy after {NUM_EPOCHS} epochs: {acc:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rf9P24EI2ikw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf9P24EI2ikw",
        "outputId": "ac367075-3e32-4f5a-ca2d-b8d5aa8953e3"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"./exp_ravdess_amp/best_ckpt\"     # pick any folder name\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)             # ⇦ weights + config\n",
        "processor.save_pretrained(OUTPUT_DIR)         # ⇦ tokenizer / feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EslBPe8boMSc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "47c99d92f7e94983b9fd2e10f34edea7",
            "1ff150767ac849a8b82adf22738cba6a",
            "2f5795d880344d09afb1b7a198b4c470",
            "dcb563112df149f89e5808bf6aacca67",
            "b46821753fa641bb98387fc3d25e83c5",
            "27e6ec61a15144dc88fe3eaa6e2d3699",
            "5f57df9f171f447c8ecaf3c6d370dccc",
            "7a73c08b785e445db34b95f1d81f1322",
            "308dba04dedc471d8ef052445ef0c91d",
            "f78b6cf434c84ebfbf077a17e924a559",
            "5c77ed09d3c04620ba10d6d13a559709",
            "a395890c678644c5ad755fca1fa2c77b",
            "7d94ccc0a44b4e85b61d34a0e60eb004",
            "e689d798f2f44024ba87688c896d9136",
            "eef5271917054746840f78334ac4093a",
            "1c20318efe5447d8a1a19d88b240f184",
            "a1f4c4b3217b41c9bba4a27808207dbf",
            "ae54ca8ae96849689e4e9ca514294baf",
            "548a278dc558456895148ef8827ec372",
            "25b1e8cfcc714355a9b792154bf45f3a",
            "4b031e020c794e5884d9434971dbf7ed",
            "b015973f7aa148fda7472ba114be1e87",
            "e5714191879d45fabfdddd35d9295f68",
            "74184063fca14de291f5ca6d0eee3a3e",
            "8f2d0f09db484add90f3ebadc78942a1",
            "4465f5cb6df6452991752837565645d8",
            "6061b6b092cc4af4a893e549b2f752d6",
            "8edd60c129bd4916a33c0c366762a3f9",
            "87a1b486daa946c597ec07a7e566110f",
            "e73c6838a1544e69af50561bdb307640",
            "e49983c0a3a344328e27d275fea33195",
            "fc423816599c4f8f91793e89f2da5496",
            "786e2cde23774c3c86068f33f09854fe"
          ]
        },
        "id": "EslBPe8boMSc",
        "outputId": "a2f72cf5-f9a1-4b89-b564-6d0cccf0e621"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spectrogram CNN‑Transformer for RAVDESS SER\n",
        "-------------------------------------------\n",
        "• Log‑mel spectrogram (128 bins, 25 ms / 10 ms)\n",
        "• 3‑layer 2‑D CNN → Transformer encoder (4 blocks, 8 heads)\n",
        "• Training: AMP, lr 3e‑5, batch 4 (+ grad‑accum 2) for 3 epochs\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path  # <— set me\n",
        "EXP_DIR    = \"./exp_ravdess_spec_tr\"\n",
        "SR         = 16_000\n",
        "N_MELS         = 128\n",
        "NUM_EPOCHS     = 20         # ★\n",
        "LR_MAX         = 5e-5        # ★ peak of 1‑cycle\n",
        "LABEL_SMOOTH   = 0.1        # ★\n",
        "WIN_LEN    = 0.025        # 25 ms\n",
        "HOP_LEN    = 0.010        # 10 ms\n",
        "SEED       = 42\n",
        "TRAIN_BS   = 16\n",
        "EVAL_BS    = 16\n",
        "GRAD_ACCUM = 2\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random, math\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- DATASET -------------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "def emotion_id(name): return int(EMO_RE.search(name).group(3)) - 1\n",
        "\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft= int(SR * WIN_LEN),\n",
        "    hop_length= int(SR * HOP_LEN),\n",
        "    n_mels=N_MELS,\n",
        ")\n",
        "db_transform = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "def wav_to_logmel(wav: torch.Tensor) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        spec = mel_spec(wav)        # [1, n_mels, T]\n",
        "        spec_db = db_transform(spec) # log‑mel\n",
        "    return spec_db                  # still [1, n_mels, T]\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True)     # mono, shape [1, L]\n",
        "    wav = wav / (wav.abs().max() + 1e-9)     # normalise\n",
        "    return wav_to_logmel(wav)               # [1, n_mels, T]\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"spec\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in test_paths]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "\n",
        "# ------------------ PADDING COLLATE -------------------\n",
        "@dataclass\n",
        "class Collate:\n",
        "    pad_value: float = -80.0\n",
        "    def __call__(self, batch):\n",
        "        specs = [b[\"spec\"].squeeze(0) for b in batch]   # [n_mels, Ti]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        max_T = max(s.shape[1] for s in specs)\n",
        "        padded = torch.stack([\n",
        "            torch.nn.functional.pad(s, (0, max_T - s.shape[1]), value=self.pad_value)\n",
        "            for s in specs\n",
        "        ])                                              # [B, n_mels, max_T]\n",
        "        return {\"spec\": padded.unsqueeze(1), \"labels\": labels}  # add channel dim\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "print(val_ld)\n",
        "\n",
        "# --------------- CNN‑TRANSFORMER MODEL ----------------\n",
        "class SpecAug(nn.Module):\n",
        "    def __init__(self, freq_mask=15, time_mask=40, p=0.5):\n",
        "        super().__init__()\n",
        "        self.f = torchaudio.transforms.FrequencyMasking(freq_mask)\n",
        "        self.t = torchaudio.transforms.TimeMasking(time_mask)\n",
        "        self.p = p\n",
        "    def forward(self, x):\n",
        "        if self.training and torch.rand(1) < self.p:\n",
        "            x = self.f(x);  x = self.t(x)\n",
        "        return x\n",
        "\n",
        "spec_aug = SpecAug()\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True) / (wav.abs().max() + 1e-9)\n",
        "    spec = wav_to_logmel(wav)             # [1, mels, T]\n",
        "    return spec_aug(spec)                 # ★ augment on CPU\n",
        "\n",
        "# ---------------- CNN‑Transformer 2.0 -------- ★\n",
        "class SpecCNNTr(nn.Module):\n",
        "    def __init__(self, n_mels=N_MELS, n_classes=8):\n",
        "        super().__init__()\n",
        "        # CNN stem with SE blocks\n",
        "        self.cnn = nn.Sequential(\n",
        "            self._block(1, 64),   # out: [B,64,mel/2,T/2]\n",
        "            self._block(64,128),\n",
        "            self._block(128,192),\n",
        "            self._block(192,256),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "        d_model = 512\n",
        "        self.proj = nn.Linear(256, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=8,\n",
        "            dim_feedforward=1024, dropout=0.3, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.cls = nn.Linear(d_model, n_classes)\n",
        "    def _block(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "    def forward(self, x):                  # x [B,1,mels,T]\n",
        "        x = self.cnn(x)                    # [B,256,F',T']\n",
        "        x = self.pool(x).squeeze(2)        # [B,256,T']\n",
        "        x = self.proj(x.transpose(1,2))    # [B,T',512]\n",
        "        x = self.transformer(x)            # [B,T',512]\n",
        "        return self.cls(x.mean(1))\n",
        "\n",
        "model = SpecCNNTr().to(device)\n",
        "\n",
        "# --------------- optimiser & 1‑cycle ------------ ★\n",
        "optimizer = AdamW(model.parameters(), lr=LR_MAX,\n",
        "                  betas=(0.9,0.98), weight_decay=1e-2, eps=1e-8)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / GRAD_ACCUM)\n",
        "total_steps     = steps_per_epoch * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR_MAX,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1, anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "train_losses, val_accuracies = [], []\n",
        "train_loss_epoch = 0\n",
        "\n",
        "\n",
        "# ------------------- TRAIN / VAL ----------------------\n",
        "def evaluate(loader):\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad(), autocast():\n",
        "        for i, batch in enumerate(loader): # Added 'i' for indexing\n",
        "            spec = batch[\"spec\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device) # Get true labels for comparison\n",
        "\n",
        "            logits = model(spec)\n",
        "            predicted_classes = logits.argmax(-1) # This is a torch.Tensor of predicted class indices\n",
        "\n",
        "            # --- CRITICAL DEBUG FOR EVALUATION ---\n",
        "            if i < 5: # Only print for the first 5 batches in the validation set\n",
        "                print(f\"\\n--- EVALUATION BATCH {i} ---\")\n",
        "                print(f\"  True Labels (Batch): {labels.cpu().numpy()}\")\n",
        "                print(f\"  Predicted Labels (Batch): {predicted_classes.cpu().numpy()}\")\n",
        "\n",
        "                # Check for direct matches\n",
        "                matches = (labels == predicted_classes).cpu().numpy()\n",
        "                print(f\"  Matches: {matches.sum()} / {len(matches)}\")\n",
        "                print(f\"  Accuracy in batch: {matches.mean():.4f}\")\n",
        "\n",
        "                print(f\"  Logits shape: {logits.shape}, Min/Max: {logits.min().item():.4f} / {logits.max().item():.4f}\")\n",
        "                print(\"------------------------------\")\n",
        "            # -------------------------------------\n",
        "\n",
        "            preds.extend(predicted_classes.cpu().tolist()) # Convert to list for extend\n",
        "            gts.extend(labels.cpu().tolist()) # Convert to list for extend\n",
        "    return accuracy_score(gts, preds)\n",
        "scaler = GradScaler()\n",
        "best_acc = 0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, batch in enumerate(pbar, 1):\n",
        "        spec = batch[\"spec\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            logits = model(spec)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels) / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        train_loss_epoch += loss.item() * GRAD_ACCUM\n",
        "        if step % GRAD_ACCUM == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            train_losses.append(train_loss_epoch / steps_per_epoch)\n",
        "            train_loss_epoch = 0\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    val_acc = evaluate(val_ld)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_cnn.pt\")\n",
        "\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n*** Training done. Final val acc: {val_acc:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uKzIkJHBxK9y",
      "metadata": {
        "id": "uKzIkJHBxK9y"
      },
      "outputs": [],
      "source": [
        "import json, numpy as np, pathlib, datetime as dt\n",
        "stats = {\"loss\": train_losses, \"val_acc\": val_accuracies}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KCpvs_zHxLbg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "KCpvs_zHxLbg",
        "outputId": "3274734f-7063-4028-bb2f-fe75d9218343"
      },
      "outputs": [],
      "source": [
        "import json, matplotlib.pyplot as plt, numpy as np, pathlib\n",
        "\n",
        "epochs = np.arange(1, len(train_losses) + 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = np.array(train_losses)\n",
        "window = 50\n",
        "\n",
        "# Compute the moving average (valid mode so we don’t pad)\n",
        "smoothed = np.convolve(loss, np.ones(window)/window, mode=\"valid\")\n",
        "\n",
        "# The x‐axis for the smoothed curve should start at “window/2” roughly\n",
        "iters = np.arange(len(smoothed)) + window//2\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iters, smoothed, label=f\"loss (smoothed, window={window})\")\n",
        "plt.xlabel(\"Training iteration\")\n",
        "plt.ylabel(\"Cross‐entropy loss\")\n",
        "plt.title(\"Smoothed training‐loss – Spectrogram CNN-Transformer\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_curve_smoothed.png\")\n",
        "plt.show()\n",
        "\n",
        "print(train_losses[-1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-KodporP1Gme",
      "metadata": {
        "id": "-KodporP1Gme"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "ckpt_dir = \"./exp_ravdess_spec_tr\"     # your HF dir\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(ckpt_dir)\n",
        "model.eval()\n",
        "\n",
        "# Save weights as a plain .pt state‑dict\n",
        "torch.save(model.state_dict(), \"best_w2v2.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ohCTqrcI3kwk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "ohCTqrcI3kwk",
        "outputId": "3e4725d2-f6bb-4fab-9fa1-f3c7dcc1ce58"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# fusion_ravdess.py\n",
        "\n",
        "\"\"\"\n",
        "Late-fusion ensemble for RAVDESS:\n",
        "    • Branch A: Wav2Vec2-base classifier\n",
        "    • Branch B: Spectrogram CNN-Transformer classifier\n",
        "    • LogisticRegression over softmax outputs (16-dim feature)\n",
        "    • Stratified validation split, softmax inputs, C=500 regularization\n",
        "    • MelSpectrogram parameters exactly match SpecCNNTr training:\n",
        "        – SR = 16 kHz, n_fft = int(16 000 × 0.025) = 400\n",
        "        – hop_length = int(16 000 × 0.010) = 160\n",
        "        – n_mels = 128\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "# ─── PATHS & CONFIGURATION ───────────────────────────────────────────────\n",
        "DATA_DIR   = path  # adjust to your environment\n",
        "W2V2_PT    = \"best_model.pt\"         # your fine-tuned Wav2Vec2 weights\n",
        "SPEC_PT    = \"best_cnn.pt\"         # your fine-tuned SpecCNNTr weights\n",
        "FUSION_OUT = \"./fusion_head2.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── LABEL UTILITY ───────────────────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in test_paths]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "print(f\"Training on {len(train_paths)} clips, validating on {len(val_paths)} clips.\")\n",
        "\n",
        "# ─── BRANCH A: Wav2Vec2-base ───────────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "\n",
        "state_w2v = torch.load(W2V2_PT, map_location=device)\n",
        "w2v2_model.load_state_dict(state_w2v)\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor @16 kHz, normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    inp = processor(wav.numpy(),\n",
        "                    sampling_rate=SR,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)  # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()         # [8]\n",
        "\n",
        "# ─── BRANCH B: Spectrogram CNN-Transformer ──────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "state_spec = torch.load(SPEC_PT, map_location=device)\n",
        "spec_model.load_state_dict(state_spec)\n",
        "spec_model.eval()\n",
        "\n",
        "# MelSpectrogram parameters exactly as used during SpecCNNTr training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 16 000 × 0.025 = 400\n",
        "    hop_length=int(SR * 0.010), # 16 000 × 0.010 = 160\n",
        "    n_mels=128                  # matches N_MELS during training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor [L], normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    # 1) Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1, 128, T']\n",
        "    spec_db = to_db(spec)               # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1, 1, 128, T']\n",
        "\n",
        "    # 2) Forward through the trained SpecCNNTr\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()      # [8]\n",
        "\n",
        "# ─── STEP 1: VERIFY EACH BRANCH ON VALIDATION SET ─────────────────────────\n",
        "def evaluate_branch(predict_fn, paths):\n",
        "    preds, gts = [], []\n",
        "    for p in paths:\n",
        "        wav, sr = torchaudio.load(p)\n",
        "        if sr != SR:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "        wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "        prob = predict_fn(wav)           # [8]\n",
        "        preds.append(int(prob.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "acc_w2v2 = evaluate_branch(probs_w2v2, val_paths)\n",
        "acc_spec = evaluate_branch(probs_spec, val_paths)\n",
        "print(f\"Wav2Vec2 branch accuracy on val:   {acc_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch accuracy on val:   {acc_spec:.4f}\")\n",
        "\n",
        "# ─── STEP 2: TRAIN LOGISTIC-REGRESSION FUSION ON SOFTMAX INPUTS ──────────\n",
        "X, y = [], []\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "    p1 = probs_w2v2(wav)           # [8]\n",
        "    p2 = probs_spec(wav)           # [8]\n",
        "    X.append(torch.cat([p1, p2]).numpy())  # 16-dim feature\n",
        "    y.append(emotion_id(str(p)))\n",
        "\n",
        "fusion = LogisticRegression(\n",
        "    C=500,                    # low L2 penalty → near-exact fit if possible\n",
        "    max_iter=2000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\"\n",
        ")\n",
        "fusion.fit(X, y)\n",
        "train_acc_lr = accuracy_score(y, fusion.predict(X))\n",
        "print(f\"LogisticRegression (train-on-val) acc = {train_acc_lr:.4f}\")\n",
        "\n",
        "with open(FUSION_OUT, \"wb\") as fout:\n",
        "    pickle.dump(fusion, fout)\n",
        "print(f\"Saved fusion head → {FUSION_OUT}\")\n",
        "\n",
        "# ─── STEP 3: SANITY CHECK ON A RANDOM VALIDATION CLIP ─────────────────────\n",
        "import numpy as np\n",
        "test_clip = random.choice(val_paths)\n",
        "wav, sr    = torchaudio.load(test_clip)\n",
        "if sr != SR:\n",
        "    wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "p1 = probs_w2v2(wav).numpy()\n",
        "p2 = probs_spec(wav).numpy()\n",
        "fused_probs = fusion.predict_proba([np.concatenate([p1, p2])])[0]\n",
        "predicted = int(fused_probs.argmax())\n",
        "true_label = emotion_id(str(test_clip))\n",
        "\n",
        "print(f\"Sanity check clip: {test_clip.name}\")\n",
        "print(f\" W2V2 predicts {int(p1.argmax())}, SpecCNNTr predicts {int(p2.argmax())}\")\n",
        "print(f\" Fusion predicts {predicted}, true = {true_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_ghUeP4zSRp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_ghUeP4zSRp",
        "outputId": "b2f5d3da-2f39-4c2f-a06d-06bce5f6ec82"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# ─── IMPORT YOUR SPEC MODEL CLASS ─────────────────────────────────────────\n",
        "\n",
        "\n",
        "# ─── CONFIGURATION ────────────────────────────────────────────────────────\n",
        "DATA_DIR   = path # adjust\n",
        "W2V2_PT    = \"best_w2v2.pt\"\n",
        "SPEC_PT    = \"cnn_model.pt\"\n",
        "FUSION_PT  = \"fusion_head.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "ALPHA      = 0.5  # for weighted average\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── UTILITY: EXTRACT EMOTION ID ───────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "_, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# ─── LOAD BRANCH A: Wav2Vec2-base ─────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2_model.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD BRANCH B: SpecCNNTr ─────────────────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "# Use the same MelSpectrogram settings from training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 0.025 s at 16 kHz → n_fft = 400\n",
        "    hop_length=int(SR * 0.010), # 0.010 s at 16 kHz → hop_length = 160\n",
        "    n_mels=128                  # exactly as in SpecCNNTr’s training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1,128,T']\n",
        "    spec_db = to_db(spec)               # [1,128,T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1,1,128,T']\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD FUSION HEAD (Logistic Regression) ──────────────────────────────\n",
        "fusion = pickle.load(open(FUSION_PT, \"rb\"))\n",
        "\n",
        "# ─── EVALUATE ALL FOUR MODELS ─────────────────────────────────────────────\n",
        "y_true, preds_w2v2, preds_spec, preds_avg, preds_lr = [], [], [], [], []\n",
        "\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # normalize\n",
        "\n",
        "    # ground truth\n",
        "    true = emotion_id(str(p))\n",
        "    y_true.append(true)\n",
        "\n",
        "    # branch A: Wav2Vec2\n",
        "    p1 = probs_w2v2(wav).numpy()    # [8]\n",
        "    pred1 = int(np.argmax(p1))\n",
        "    preds_w2v2.append(pred1)\n",
        "\n",
        "    # branch B: SpecCNNTr\n",
        "    p2 = probs_spec(wav).numpy()    # [8]\n",
        "    pred2 = int(np.argmax(p2))\n",
        "    preds_spec.append(pred2)\n",
        "\n",
        "    # weighted average (alpha = 0.5)\n",
        "    fused_probs = ALPHA * p1 + (1 - ALPHA) * p2\n",
        "    preds_avg.append(int(np.argmax(fused_probs)))\n",
        "\n",
        "    # logistic regression fusion\n",
        "    fused_lr = fusion.predict([np.concatenate([p1, p2])])[0]\n",
        "    preds_lr.append(int(fused_lr))\n",
        "\n",
        "# ─── COMPUTE F1 SCORES (MACRO) ─────────────────────────────────────────────\n",
        "f1_w2v2 = f1_score(y_true, preds_w2v2, average=\"macro\")\n",
        "f1_spec = f1_score(y_true, preds_spec, average=\"macro\")\n",
        "f1_avg  = f1_score(y_true, preds_avg,  average=\"macro\")\n",
        "f1_lr   = f1_score(y_true, preds_lr,   average=\"macro\")\n",
        "# ─── VALIDATION LOSS CALCULATION ─────────────────────────────────────────────\n",
        "\n",
        "# We’ll accumulate total loss and then divide by number of samples:\n",
        "total_w2v2_loss = 0.0\n",
        "total_spec_loss = 0.0\n",
        "total_avg_loss  = 0.0\n",
        "total_lr_loss   = 0.0\n",
        "n = len(val_paths)\n",
        "\n",
        "for p in val_paths:\n",
        "    # Load & normalize waveform\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "\n",
        "    true_label = emotion_id(str(p))\n",
        "    label_tensor = torch.tensor([true_label]).long()  # shape [1]\n",
        "\n",
        "    # ---- Wav2Vec2 branch loss ----\n",
        "    # Get raw logits from Wav2Vec2 (before softmax)\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits_w2v = w2v2_model(**inp).logits.squeeze(0)  # [8] on device\n",
        "    loss_w2v2 = F.cross_entropy(logits_w2v.unsqueeze(0), label_tensor.to(device))\n",
        "    total_w2v2_loss += loss_w2v2.item()\n",
        "\n",
        "    # ---- SpecCNNTr branch loss ----\n",
        "    # Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))        # [1, 128, T']\n",
        "    spec_db = to_db(spec)                    # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)   # [1, 1, 128, T']\n",
        "    logits_spec = spec_model(spec_db).squeeze(0)  # [8] on device\n",
        "    loss_spec = F.cross_entropy(logits_spec.unsqueeze(0), label_tensor.to(device))\n",
        "    total_spec_loss += loss_spec.item()\n",
        "\n",
        "    # ---- Weighted-average ensemble loss ----\n",
        "    # We already have p1, p2 as probabilities, but we need fused_probs\n",
        "    with torch.no_grad():\n",
        "        # Recompute probabilities on CPU\n",
        "        p1 = F.softmax(logits_w2v, dim=-1).cpu().numpy()   # [8]\n",
        "        p2 = F.softmax(logits_spec, dim=-1).cpu().numpy()  # [8]\n",
        "        fused_probs = ALPHA * p1 + (1 - ALPHA) * p2         # [8] on CPU\n",
        "\n",
        "    # Cross-entropy for weighted average: -log(fused_probs[true_label])\n",
        "    eps = 1e-9  # avoid log(0)\n",
        "    total_avg_loss += -np.log(fused_probs[true_label] + eps)\n",
        "\n",
        "    # ---- Logistic Regression fusion loss ----\n",
        "    # Use fusion.predict_proba to get probabilities, then compute -log for true class\n",
        "    lr_probs = fusion.predict_proba([np.concatenate([p1, p2])])  # [8]\n",
        "    # print(lr_probs)\n",
        "    # total_lr_loss += -np.log(lr_probs[true_label] + eps)\n",
        "\n",
        "# Compute average loss over validation set\n",
        "val_loss_w2v2 = total_w2v2_loss / n\n",
        "val_loss_spec = total_spec_loss / n\n",
        "val_loss_avg  = total_avg_loss  / n\n",
        "# val_loss_lr   = total_lr_loss   / n\n",
        "\n",
        "# ─── PRINT VALIDATION LOSSES ─────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch   –  Val Loss = {val_loss_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch  –  Val Loss = {val_loss_spec:.4f}\")\n",
        "print(f\"Weighted Avg       –  Val Loss = {val_loss_avg:.4f}\")\n",
        "# print(f\"LR Fusion          –  Val Loss = {val_loss_lr:.4f}\")\n",
        "# ─── PRINT RESULTS ─────────────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch  –  Val Accuracy = {accuracy_score(y_true, preds_w2v2):.4f},  F1 (macro) = {f1_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch –  Val Accuracy = {accuracy_score(y_true, preds_spec):.4f},  F1 (macro) = {f1_spec:.4f}\")\n",
        "print(f\"Weighted Avg      –  Val Accuracy = {accuracy_score(y_true, preds_avg):.4f},  F1 (macro) = {f1_avg:.4f}\")\n",
        "print(f\"LR Fusion         –  Val Accuracy = {accuracy_score(y_true, preds_lr):.4f},  F1 (macro) = {f1_lr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8QMwZzs84ozg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QMwZzs84ozg",
        "outputId": "05000d07-cb02-4d1f-8985-130b292f6a46"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Import your spectrogram model class and the wav→logmel helper\n",
        "\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────\n",
        "DATA_DIR      = path   # adjust\n",
        "W2V2_PT       = \"best_w2v2.pt\"\n",
        "SPEC_PT       = \"cnn_model.pt\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ── LABEL EXTRACTION ───────────────────────────────────────────\n",
        "import re\n",
        "EMO_RE = re.compile(r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "                    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1  # 0–7\n",
        "\n",
        "# ── COLLECT & SPLIT ───────────────────────────────────────────\n",
        "all_wavs = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_wavs,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=[emotion_id(str(p)) for p in all_wavs],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# ── LOAD Wav2Vec2 BRANCH ───────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_w2v2(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(\n",
        "        wav_tensor.numpy(),\n",
        "        sampling_rate=SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "    logits = w2v2(**inp).logits  # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)  # [8]\n",
        "\n",
        "# ── LOAD SPECTROGRAM BRANCH ───────────────────────────────────\n",
        "\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_spec(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    spec = wav_to_logmel(wav_tensor.unsqueeze(0))     # [1, n_mels, T]\n",
        "    spec = spec.unsqueeze(0).to(device)                # [1, 1, n_mels, T]\n",
        "    logits = spec_model(spec)                          # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)\n",
        "\n",
        "# ── EVALUATION UTILS ─────────────────────────────────────────\n",
        "def load_and_normalize(path: Path) -> torch.Tensor:\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "    return wav\n",
        "\n",
        "def evaluate_ensemble(alpha: float, val_files):\n",
        "    preds, gts = [], []\n",
        "    for p in val_files:\n",
        "        wav = load_and_normalize(p)\n",
        "        p1 = logits_w2v2(wav)   # [8]\n",
        "        p2 = logits_spec(wav)   # [8]\n",
        "        fused = alpha * p1 + (1.0 - alpha) * p2\n",
        "        preds.append(int(fused.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "# ── SEARCH BEST ALPHA ─────────────────────────────────────────\n",
        "best_alpha, best_acc = 0.0, 0.0\n",
        "for alpha in np.linspace(0.0, 1.0, 3):  # 0.0, 0.05, ..., 1.0\n",
        "    acc = evaluate_ensemble(alpha, val_paths)\n",
        "    if acc > best_acc:\n",
        "        best_acc, best_alpha = acc, alpha\n",
        "\n",
        "print(f\"Best weighted‐avg α = {best_alpha:.2f}, Val Acc = {best_acc:.4f}\")\n",
        "\n",
        "# ── OPTIONALLY: PRINT A FEW SAMPLE PREDICTIONS ─────────────────\n",
        "for i, p in enumerate(random.sample(val_paths, 5), 1):\n",
        "    wav = load_and_normalize(p)\n",
        "    p1 = logits_w2v2(wav).argmax().item()\n",
        "    p2 = logits_spec(wav).argmax().item()\n",
        "    fused = (best_alpha * logits_w2v2(wav) + (1.0 - best_alpha) * logits_spec(wav)).argmax().item()\n",
        "    true = emotion_id(str(p))\n",
        "    print(f\"{i}) {p.name} → W2V2={p1}, SPEC={p2}, FUSED={fused}, TRUE={true}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03eaba4bd20a4b58bd0021c812e6d6de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "105188ef7f194f99b19fafb62e9f033d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c20318efe5447d8a1a19d88b240f184": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "1ff150767ac849a8b82adf22738cba6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e6ec61a15144dc88fe3eaa6e2d3699",
            "placeholder": "​",
            "style": "IPY_MODEL_5f57df9f171f447c8ecaf3c6d370dccc",
            "value": ""
          }
        },
        "2568e99ff5284c0ca5462883f217373a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25b1e8cfcc714355a9b792154bf45f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25f74e6963384a59b117508a57930e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27e6ec61a15144dc88fe3eaa6e2d3699": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5795d880344d09afb1b7a198b4c470": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a73c08b785e445db34b95f1d81f1322",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_308dba04dedc471d8ef052445ef0c91d",
            "value": 72
          }
        },
        "2f6417545f124023be3fb3af07612d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "308dba04dedc471d8ef052445ef0c91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30f1d6f5b7364d2381a39912a82ecd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f321fa64bc144b4c931a70ea83babb94",
            "max": 288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65447255395e467c8a8825d6be011812",
            "value": 288
          }
        },
        "388151add624408ca0f4d21fe59db0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2db9c68aee54f3ebbcd87483d9dda2c",
            "placeholder": "​",
            "style": "IPY_MODEL_75b7bf0245fb4b0d972cd60f08e73016",
            "value": ""
          }
        },
        "3cf0eb6b1b4a409181b435eaddfb280e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "424395352d6548f9938c8dd1322bcd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a9b96f5274b4f6b806f6172d2c1109a",
            "placeholder": "​",
            "style": "IPY_MODEL_2568e99ff5284c0ca5462883f217373a",
            "value": ""
          }
        },
        "4465f5cb6df6452991752837565645d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc423816599c4f8f91793e89f2da5496",
            "placeholder": "​",
            "style": "IPY_MODEL_786e2cde23774c3c86068f33f09854fe",
            "value": "Epoch 3/20:   8% 6/72 [00:02&lt;00:26,  2.45it/s, loss=1.7908, lr=5e-5]"
          }
        },
        "4501d091a54941d79c574a88fed80359": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47c99d92f7e94983b9fd2e10f34edea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ff150767ac849a8b82adf22738cba6a",
              "IPY_MODEL_2f5795d880344d09afb1b7a198b4c470",
              "IPY_MODEL_dcb563112df149f89e5808bf6aacca67"
            ],
            "layout": "IPY_MODEL_b46821753fa641bb98387fc3d25e83c5"
          }
        },
        "4a9b96f5274b4f6b806f6172d2c1109a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b031e020c794e5884d9434971dbf7ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "548a278dc558456895148ef8827ec372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c77ed09d3c04620ba10d6d13a559709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f57df9f171f447c8ecaf3c6d370dccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6061b6b092cc4af4a893e549b2f752d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "640d4bb42cdf4b4ab1fb649c435f6a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65447255395e467c8a8825d6be011812": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6773e6823d244a1fa1fceea6aeeb5b20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "6fd0893b6cb748c0b814f11b6d9d9cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71fa3b0f11bf4785ae1a3326f07783e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca50560a4ec04e0ca45fffa23cf22ddb",
              "IPY_MODEL_78346f06fb5544c785789517f5521d1e",
              "IPY_MODEL_f6dbb4c8e0cd4aaca81be647b79b01b5"
            ],
            "layout": "IPY_MODEL_6773e6823d244a1fa1fceea6aeeb5b20"
          }
        },
        "74184063fca14de291f5ca6d0eee3a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8edd60c129bd4916a33c0c366762a3f9",
            "placeholder": "​",
            "style": "IPY_MODEL_87a1b486daa946c597ec07a7e566110f",
            "value": ""
          }
        },
        "75b7bf0245fb4b0d972cd60f08e73016": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78346f06fb5544c785789517f5521d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f6417545f124023be3fb3af07612d2d",
            "max": 288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fd0893b6cb748c0b814f11b6d9d9cd9",
            "value": 288
          }
        },
        "786e2cde23774c3c86068f33f09854fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a73c08b785e445db34b95f1d81f1322": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d94ccc0a44b4e85b61d34a0e60eb004": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f4c4b3217b41c9bba4a27808207dbf",
            "placeholder": "​",
            "style": "IPY_MODEL_ae54ca8ae96849689e4e9ca514294baf",
            "value": ""
          }
        },
        "82ec52e11c6a4d629af2946aa3b0b444": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a1b486daa946c597ec07a7e566110f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89805eab08e54e878dfaa806215b48e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8edd60c129bd4916a33c0c366762a3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f2d0f09db484add90f3ebadc78942a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e73c6838a1544e69af50561bdb307640",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e49983c0a3a344328e27d275fea33195",
            "value": 6
          }
        },
        "a1f4c4b3217b41c9bba4a27808207dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2789dae1eb24dbf900ad967ddcd5407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105188ef7f194f99b19fafb62e9f033d",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf0eb6b1b4a409181b435eaddfb280e",
            "value": "Epoch 3/3: 100% 288/288 [01:07&lt;00:00,  4.22it/s, loss=0.8330, lr=0]"
          }
        },
        "a395890c678644c5ad755fca1fa2c77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d94ccc0a44b4e85b61d34a0e60eb004",
              "IPY_MODEL_e689d798f2f44024ba87688c896d9136",
              "IPY_MODEL_eef5271917054746840f78334ac4093a"
            ],
            "layout": "IPY_MODEL_1c20318efe5447d8a1a19d88b240f184"
          }
        },
        "ae54ca8ae96849689e4e9ca514294baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b015973f7aa148fda7472ba114be1e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b46821753fa641bb98387fc3d25e83c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "b4a0e4fa7edb463fad4e63246f0e3254": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca50560a4ec04e0ca45fffa23cf22ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6124a311024052ab90a2d9181cb075",
            "placeholder": "​",
            "style": "IPY_MODEL_25f74e6963384a59b117508a57930e8e",
            "value": ""
          }
        },
        "cac18347c55a4981b5ab1fb5fdae3287": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_424395352d6548f9938c8dd1322bcd0f",
              "IPY_MODEL_30f1d6f5b7364d2381a39912a82ecd36",
              "IPY_MODEL_cb2a7a7fc56f4a8186c90c3d18e1b76b"
            ],
            "layout": "IPY_MODEL_03eaba4bd20a4b58bd0021c812e6d6de"
          }
        },
        "cb2a7a7fc56f4a8186c90c3d18e1b76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640d4bb42cdf4b4ab1fb649c435f6a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_89805eab08e54e878dfaa806215b48e9",
            "value": "Epoch 2/3: 100% 288/288 [01:07&lt;00:00,  4.15it/s, loss=0.5071, lr=3.7e-5]"
          }
        },
        "ce6124a311024052ab90a2d9181cb075": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4019cbb45464a8abba16fd10f0f177d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a0e4fa7edb463fad4e63246f0e3254",
            "max": 288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8a4d47d5d354566b3ccb8e8623b6e5f",
            "value": 288
          }
        },
        "d7d4e3a314514c1daf68d9a1e4939d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_388151add624408ca0f4d21fe59db0a8",
              "IPY_MODEL_d4019cbb45464a8abba16fd10f0f177d",
              "IPY_MODEL_a2789dae1eb24dbf900ad967ddcd5407"
            ],
            "layout": "IPY_MODEL_d98d219351a6415a8663fcfba8142db2"
          }
        },
        "d8a4d47d5d354566b3ccb8e8623b6e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d98d219351a6415a8663fcfba8142db2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "dcb563112df149f89e5808bf6aacca67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78b6cf434c84ebfbf077a17e924a559",
            "placeholder": "​",
            "style": "IPY_MODEL_5c77ed09d3c04620ba10d6d13a559709",
            "value": "Epoch 1/20: 100% 72/72 [00:30&lt;00:00,  2.37it/s, loss=1.9284, lr=2.65e-5]"
          }
        },
        "e2db9c68aee54f3ebbcd87483d9dda2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e49983c0a3a344328e27d275fea33195": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5714191879d45fabfdddd35d9295f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74184063fca14de291f5ca6d0eee3a3e",
              "IPY_MODEL_8f2d0f09db484add90f3ebadc78942a1",
              "IPY_MODEL_4465f5cb6df6452991752837565645d8"
            ],
            "layout": "IPY_MODEL_6061b6b092cc4af4a893e549b2f752d6"
          }
        },
        "e689d798f2f44024ba87688c896d9136": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_548a278dc558456895148ef8827ec372",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25b1e8cfcc714355a9b792154bf45f3a",
            "value": 72
          }
        },
        "e73c6838a1544e69af50561bdb307640": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef5271917054746840f78334ac4093a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b031e020c794e5884d9434971dbf7ed",
            "placeholder": "​",
            "style": "IPY_MODEL_b015973f7aa148fda7472ba114be1e87",
            "value": "Epoch 2/20: 100% 72/72 [00:29&lt;00:00,  2.55it/s, loss=1.9563, lr=5e-5]"
          }
        },
        "f321fa64bc144b4c931a70ea83babb94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6dbb4c8e0cd4aaca81be647b79b01b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ec52e11c6a4d629af2946aa3b0b444",
            "placeholder": "​",
            "style": "IPY_MODEL_4501d091a54941d79c574a88fed80359",
            "value": "Epoch 1/3: 100% 288/288 [01:08&lt;00:00,  4.29it/s, loss=1.9053, lr=7.4e-5]"
          }
        },
        "f78b6cf434c84ebfbf077a17e924a559": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc423816599c4f8f91793e89f2da5496": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
