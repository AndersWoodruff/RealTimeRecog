{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4adade77",
      "metadata": {
        "id": "4adade77"
      },
      "source": [
        "### EMOVOICE: Real-time Speech Emotion Recognition Using Raw Audio Features and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02c13ea",
      "metadata": {
        "id": "b02c13ea"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kagglehub librosa numpy noisereduce evaluate transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wH9-oGOhk5D",
        "outputId": "66c639ba-a93e-4fbf-b719-cd7fd4dbe6dc"
      },
      "id": "_wH9-oGOhk5D",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting noisereduce\n",
            "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from noisereduce) (3.10.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: torch<2.7,>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, noisereduce, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.3 noisereduce-3.0.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22f82f34",
      "metadata": {
        "id": "22f82f34"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torchaudio as ta\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch as t\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "from typing import Optional, Callable\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
      "metadata": {
        "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f38aa2a-5abe-4599-c51c-1205705800f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "path = ('/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24')\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Wav2Vec2‑base on RAVDESS with AMP, lr = 3e‑5,\n",
        "freeze encoder epoch 0, unfreeze top‑4 layers afterwards.\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path\n",
        "EXP_DIR    = \"./exp_ravdess_amp\"\n",
        "SR         = 16_000\n",
        "SEED       = 40\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "TRAIN_BS   = 4                   # GPU batch\n",
        "EVAL_BS    = 4\n",
        "GRAD_ACCUM = 2                   # 4×2 ⇒ eff 8\n",
        "LR         =  0.0001\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------- DATASET ----------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(name: str) -> int:\n",
        "    return int(EMO_RE.search(name).group(3)) - 1          # 0‑7\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "    return wav.clamp_(-0.99, 0.99)\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"wav\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "@dataclass\n",
        "class Collate:\n",
        "    def __call__(self, batch):\n",
        "        audio  = [b[\"wav\"].cpu().numpy() for b in batch]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        proc   = processor(audio, sampling_rate=SR,\n",
        "                           padding=True, return_tensors=\"pt\")\n",
        "        proc[\"labels\"] = labels\n",
        "        return proc\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "\n",
        "# ----------------------- MODEL ------------------------\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=8,\n",
        ").to(device)\n",
        "model.gradient_checkpointing_enable()\n",
        "model.freeze_feature_encoder()       # frozen for epoch 0\n",
        "\n",
        "# -------------------- OPT & SCHED ---------------------\n",
        "optimizer = Adam(model.parameters(), lr=LR,\n",
        "                  betas=(0.9,0.999), eps=1e-8)\n",
        "\n",
        "total_steps = (len(train_ld) // GRAD_ACCUM) * NUM_EPOCHS\n",
        "warm_steps  = int(0.1 * total_steps)\n",
        "scheduler   = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warm_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "# ---------------- ENABLE AMP --------------------------\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# -------------- TRAIN / VALIDATE ----------------------\n",
        "def evaluate():\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_ld:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            preds.extend(logits.argmax(-1).cpu())\n",
        "            gts.extend(batch[\"labels\"].cpu())\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "step_global = 0\n",
        "train_loss_history   = []   # step‑level\n",
        "val_acc_history      = []   # epoch‑level\n",
        "best_acc             = 0.0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    # # --------- unfreeze top‑4 layers after epoch 0 ----------\n",
        "    # if epoch == 2:\n",
        "    #     for layer in model.wav2vec2.encoder.layer[-4:]:\n",
        "    #         for p in layer.parameters():\n",
        "    #             p.requires_grad = True\n",
        "    #     print(\"Unfroze top‑4 encoder layers.\")\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step_local, batch in enumerate(pbar, 1):\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = model(**batch).loss / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if step_local % GRAD_ACCUM == 0:\n",
        "            train_loss_history.append(loss.item() * GRAD_ACCUM)\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            step_global += 1\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    acc = evaluate()\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {acc:.4f}\")\n",
        "    val_acc_history.append(acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_model.pt\")\n",
        "\n",
        "print(f\"\\n*** Final accuracy after {NUM_EPOCHS} epochs: {acc:.4f} ***\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686,
          "referenced_widgets": [
            "71fa3b0f11bf4785ae1a3326f07783e4",
            "ca50560a4ec04e0ca45fffa23cf22ddb",
            "78346f06fb5544c785789517f5521d1e",
            "f6dbb4c8e0cd4aaca81be647b79b01b5",
            "6773e6823d244a1fa1fceea6aeeb5b20",
            "ce6124a311024052ab90a2d9181cb075",
            "25f74e6963384a59b117508a57930e8e",
            "2f6417545f124023be3fb3af07612d2d",
            "6fd0893b6cb748c0b814f11b6d9d9cd9",
            "82ec52e11c6a4d629af2946aa3b0b444",
            "4501d091a54941d79c574a88fed80359",
            "cac18347c55a4981b5ab1fb5fdae3287",
            "424395352d6548f9938c8dd1322bcd0f",
            "30f1d6f5b7364d2381a39912a82ecd36",
            "cb2a7a7fc56f4a8186c90c3d18e1b76b",
            "03eaba4bd20a4b58bd0021c812e6d6de",
            "4a9b96f5274b4f6b806f6172d2c1109a",
            "2568e99ff5284c0ca5462883f217373a",
            "f321fa64bc144b4c931a70ea83babb94",
            "65447255395e467c8a8825d6be011812",
            "640d4bb42cdf4b4ab1fb649c435f6a3b",
            "89805eab08e54e878dfaa806215b48e9",
            "d7d4e3a314514c1daf68d9a1e4939d70",
            "388151add624408ca0f4d21fe59db0a8",
            "d4019cbb45464a8abba16fd10f0f177d",
            "a2789dae1eb24dbf900ad967ddcd5407",
            "d98d219351a6415a8663fcfba8142db2",
            "e2db9c68aee54f3ebbcd87483d9dda2c",
            "75b7bf0245fb4b0d972cd60f08e73016",
            "b4a0e4fa7edb463fad4e63246f0e3254",
            "d8a4d47d5d354566b3ccb8e8623b6e5f",
            "105188ef7f194f99b19fafb62e9f033d",
            "3cf0eb6b1b4a409181b435eaddfb280e"
          ]
        },
        "id": "YFJ7pTHrxAzU",
        "outputId": "1c2572ce-6cfc-4cf0-b917-5a253ebac41e"
      },
      "id": "YFJ7pTHrxAzU",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-65-b4de955f0728>:109: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/3:   0% 0/288 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71fa3b0f11bf4785ae1a3326f07783e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-b4de955f0728>:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Validation accuracy after epoch 1: 0.3924\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/3:   0% 0/288 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cac18347c55a4981b5ab1fb5fdae3287"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-b4de955f0728>:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Validation accuracy after epoch 2: 0.6736\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/3:   0% 0/288 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7d4e3a314514c1daf68d9a1e4939d70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-b4de955f0728>:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Validation accuracy after epoch 3: 0.8160\n",
            "\n",
            "*** Final accuracy after 3 epochs: 0.8160 ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"./exp_ravdess_amp/best_ckpt\"     # pick any folder name\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)             # ⇦ weights + config\n",
        "processor.save_pretrained(OUTPUT_DIR)         # ⇦ tokenizer / feature extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf9P24EI2ikw",
        "outputId": "ac367075-3e32-4f5a-ca2d-b8d5aa8953e3"
      },
      "id": "Rf9P24EI2ikw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spectrogram CNN‑Transformer for RAVDESS SER\n",
        "-------------------------------------------\n",
        "• Log‑mel spectrogram (128 bins, 25 ms / 10 ms)\n",
        "• 3‑layer 2‑D CNN → Transformer encoder (4 blocks, 8 heads)\n",
        "• Training: AMP, lr 3e‑5, batch 4 (+ grad‑accum 2) for 3 epochs\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path  # <— set me\n",
        "EXP_DIR    = \"./exp_ravdess_spec_tr\"\n",
        "SR         = 16_000\n",
        "N_MELS         = 128\n",
        "NUM_EPOCHS     = 20         # ★\n",
        "LR_MAX         = 5e-5        # ★ peak of 1‑cycle\n",
        "LABEL_SMOOTH   = 0.1        # ★\n",
        "WIN_LEN    = 0.025        # 25 ms\n",
        "HOP_LEN    = 0.010        # 10 ms\n",
        "SEED       = 42\n",
        "TRAIN_BS   = 16\n",
        "EVAL_BS    = 16\n",
        "GRAD_ACCUM = 2\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random, math\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- DATASET -------------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "def emotion_id(name): return int(EMO_RE.search(name).group(3)) - 1\n",
        "\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft= int(SR * WIN_LEN),\n",
        "    hop_length= int(SR * HOP_LEN),\n",
        "    n_mels=N_MELS,\n",
        ")\n",
        "db_transform = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "def wav_to_logmel(wav: torch.Tensor) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        spec = mel_spec(wav)        # [1, n_mels, T]\n",
        "        spec_db = db_transform(spec) # log‑mel\n",
        "    return spec_db                  # still [1, n_mels, T]\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True)     # mono, shape [1, L]\n",
        "    wav = wav / (wav.abs().max() + 1e-9)     # normalise\n",
        "    return wav_to_logmel(wav)               # [1, n_mels, T]\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"spec\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in test_paths]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "\n",
        "# ------------------ PADDING COLLATE -------------------\n",
        "@dataclass\n",
        "class Collate:\n",
        "    pad_value: float = -80.0\n",
        "    def __call__(self, batch):\n",
        "        specs = [b[\"spec\"].squeeze(0) for b in batch]   # [n_mels, Ti]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        max_T = max(s.shape[1] for s in specs)\n",
        "        padded = torch.stack([\n",
        "            torch.nn.functional.pad(s, (0, max_T - s.shape[1]), value=self.pad_value)\n",
        "            for s in specs\n",
        "        ])                                              # [B, n_mels, max_T]\n",
        "        return {\"spec\": padded.unsqueeze(1), \"labels\": labels}  # add channel dim\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "print(val_ld)\n",
        "\n",
        "# --------------- CNN‑TRANSFORMER MODEL ----------------\n",
        "class SpecAug(nn.Module):\n",
        "    def __init__(self, freq_mask=15, time_mask=40, p=0.5):\n",
        "        super().__init__()\n",
        "        self.f = torchaudio.transforms.FrequencyMasking(freq_mask)\n",
        "        self.t = torchaudio.transforms.TimeMasking(time_mask)\n",
        "        self.p = p\n",
        "    def forward(self, x):\n",
        "        if self.training and torch.rand(1) < self.p:\n",
        "            x = self.f(x);  x = self.t(x)\n",
        "        return x\n",
        "\n",
        "spec_aug = SpecAug()\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True) / (wav.abs().max() + 1e-9)\n",
        "    spec = wav_to_logmel(wav)             # [1, mels, T]\n",
        "    return spec_aug(spec)                 # ★ augment on CPU\n",
        "\n",
        "# ---------------- CNN‑Transformer 2.0 -------- ★\n",
        "class SpecCNNTr(nn.Module):\n",
        "    def __init__(self, n_mels=N_MELS, n_classes=8):\n",
        "        super().__init__()\n",
        "        # CNN stem with SE blocks\n",
        "        self.cnn = nn.Sequential(\n",
        "            self._block(1, 64),   # out: [B,64,mel/2,T/2]\n",
        "            self._block(64,128),\n",
        "            self._block(128,192),\n",
        "            self._block(192,256),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "        d_model = 512\n",
        "        self.proj = nn.Linear(256, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=8,\n",
        "            dim_feedforward=1024, dropout=0.3, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.cls = nn.Linear(d_model, n_classes)\n",
        "    def _block(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "    def forward(self, x):                  # x [B,1,mels,T]\n",
        "        x = self.cnn(x)                    # [B,256,F',T']\n",
        "        x = self.pool(x).squeeze(2)        # [B,256,T']\n",
        "        x = self.proj(x.transpose(1,2))    # [B,T',512]\n",
        "        x = self.transformer(x)            # [B,T',512]\n",
        "        return self.cls(x.mean(1))\n",
        "\n",
        "model = SpecCNNTr().to(device)\n",
        "\n",
        "# --------------- optimiser & 1‑cycle ------------ ★\n",
        "optimizer = AdamW(model.parameters(), lr=LR_MAX,\n",
        "                  betas=(0.9,0.98), weight_decay=1e-2, eps=1e-8)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / GRAD_ACCUM)\n",
        "total_steps     = steps_per_epoch * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR_MAX,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1, anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "train_losses, val_accuracies = [], []\n",
        "train_loss_epoch = 0\n",
        "\n",
        "\n",
        "# ------------------- TRAIN / VAL ----------------------\n",
        "def evaluate(loader):\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad(), autocast():\n",
        "        for i, batch in enumerate(loader): # Added 'i' for indexing\n",
        "            spec = batch[\"spec\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device) # Get true labels for comparison\n",
        "\n",
        "            logits = model(spec)\n",
        "            predicted_classes = logits.argmax(-1) # This is a torch.Tensor of predicted class indices\n",
        "\n",
        "            # --- CRITICAL DEBUG FOR EVALUATION ---\n",
        "            if i < 5: # Only print for the first 5 batches in the validation set\n",
        "                print(f\"\\n--- EVALUATION BATCH {i} ---\")\n",
        "                print(f\"  True Labels (Batch): {labels.cpu().numpy()}\")\n",
        "                print(f\"  Predicted Labels (Batch): {predicted_classes.cpu().numpy()}\")\n",
        "\n",
        "                # Check for direct matches\n",
        "                matches = (labels == predicted_classes).cpu().numpy()\n",
        "                print(f\"  Matches: {matches.sum()} / {len(matches)}\")\n",
        "                print(f\"  Accuracy in batch: {matches.mean():.4f}\")\n",
        "\n",
        "                print(f\"  Logits shape: {logits.shape}, Min/Max: {logits.min().item():.4f} / {logits.max().item():.4f}\")\n",
        "                print(\"------------------------------\")\n",
        "            # -------------------------------------\n",
        "\n",
        "            preds.extend(predicted_classes.cpu().tolist()) # Convert to list for extend\n",
        "            gts.extend(labels.cpu().tolist()) # Convert to list for extend\n",
        "    return accuracy_score(gts, preds)\n",
        "scaler = GradScaler()\n",
        "best_acc = 0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, batch in enumerate(pbar, 1):\n",
        "        spec = batch[\"spec\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            logits = model(spec)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels) / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        train_loss_epoch += loss.item() * GRAD_ACCUM\n",
        "        if step % GRAD_ACCUM == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            train_losses.append(train_loss_epoch / steps_per_epoch)\n",
        "            train_loss_epoch = 0\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    val_acc = evaluate(val_ld)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_cnn.pt\")\n",
        "\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n*** Training done. Final val acc: {val_acc:.4f} ***\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "47c99d92f7e94983b9fd2e10f34edea7",
            "1ff150767ac849a8b82adf22738cba6a",
            "2f5795d880344d09afb1b7a198b4c470",
            "dcb563112df149f89e5808bf6aacca67",
            "b46821753fa641bb98387fc3d25e83c5",
            "27e6ec61a15144dc88fe3eaa6e2d3699",
            "5f57df9f171f447c8ecaf3c6d370dccc",
            "7a73c08b785e445db34b95f1d81f1322",
            "308dba04dedc471d8ef052445ef0c91d",
            "f78b6cf434c84ebfbf077a17e924a559",
            "5c77ed09d3c04620ba10d6d13a559709",
            "a395890c678644c5ad755fca1fa2c77b",
            "7d94ccc0a44b4e85b61d34a0e60eb004",
            "e689d798f2f44024ba87688c896d9136",
            "eef5271917054746840f78334ac4093a",
            "1c20318efe5447d8a1a19d88b240f184",
            "a1f4c4b3217b41c9bba4a27808207dbf",
            "ae54ca8ae96849689e4e9ca514294baf",
            "548a278dc558456895148ef8827ec372",
            "25b1e8cfcc714355a9b792154bf45f3a",
            "4b031e020c794e5884d9434971dbf7ed",
            "b015973f7aa148fda7472ba114be1e87",
            "e5714191879d45fabfdddd35d9295f68",
            "74184063fca14de291f5ca6d0eee3a3e",
            "8f2d0f09db484add90f3ebadc78942a1",
            "4465f5cb6df6452991752837565645d8",
            "6061b6b092cc4af4a893e549b2f752d6",
            "8edd60c129bd4916a33c0c366762a3f9",
            "87a1b486daa946c597ec07a7e566110f",
            "e73c6838a1544e69af50561bdb307640",
            "e49983c0a3a344328e27d275fea33195",
            "fc423816599c4f8f91793e89f2da5496",
            "786e2cde23774c3c86068f33f09854fe"
          ]
        },
        "id": "EslBPe8boMSc",
        "outputId": "a2f72cf5-f9a1-4b89-b564-6d0cccf0e621"
      },
      "id": "EslBPe8boMSc",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val label counts: Counter({1: 157, 5: 155, 2: 154, 3: 153, 7: 153, 4: 152, 6: 150, 0: 78})\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x791036e9bed0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n",
            "<ipython-input-90-f51e0e028915>:216: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/20:   0% 0/72 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47c99d92f7e94983b9fd2e10f34edea7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-f51e0e028915>:227: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-90-f51e0e028915>:190: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EVALUATION BATCH 0 ---\n",
            "  True Labels (Batch): [2 7 7 6 2 5 4 4 7 0 1 0 6 4 2 3]\n",
            "  Predicted Labels (Batch): [1 5 7 4 5 1 3 7 4 5 1 7 7 7 5 7]\n",
            "  Matches: 2 / 16\n",
            "  Accuracy in batch: 0.1250\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -1.4707 / 0.9292\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 1 ---\n",
            "  True Labels (Batch): [0 1 3 0 1 4 2 7 4 2 2 7 0 0 1 1]\n",
            "  Predicted Labels (Batch): [1 1 4 1 6 7 5 5 4 4 4 7 1 5 1 6]\n",
            "  Matches: 4 / 16\n",
            "  Accuracy in batch: 0.2500\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -1.4355 / 1.6689\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 2 ---\n",
            "  True Labels (Batch): [3 3 7 5 5 3 1 4 3 7 2 4 4 4 2 7]\n",
            "  Predicted Labels (Batch): [1 5 7 1 7 5 1 6 1 7 1 4 6 2 7 7]\n",
            "  Matches: 5 / 16\n",
            "  Accuracy in batch: 0.3125\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -1.4404 / 1.1963\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 3 ---\n",
            "  True Labels (Batch): [1 5 7 3 5 4 4 2 0 1 7 7 3 7 1 0]\n",
            "  Predicted Labels (Batch): [1 7 7 1 5 7 4 1 5 1 7 7 7 7 1 7]\n",
            "  Matches: 9 / 16\n",
            "  Accuracy in batch: 0.5625\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -1.6523 / 1.6836\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 4 ---\n",
            "  True Labels (Batch): [4 2 7 3 4 4 1 3 1 4 2 4 4 5 5 5]\n",
            "  Predicted Labels (Batch): [7 7 7 1 5 4 1 7 7 1 7 7 7 7 7 1]\n",
            "  Matches: 3 / 16\n",
            "  Accuracy in batch: 0.1875\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -1.5596 / 1.3379\n",
            "------------------------------\n",
            "→ Validation accuracy after epoch 1: 0.2812\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/20:   0% 0/72 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a395890c678644c5ad755fca1fa2c77b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-f51e0e028915>:227: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-90-f51e0e028915>:190: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- EVALUATION BATCH 0 ---\n",
            "  True Labels (Batch): [2 7 7 6 2 5 4 4 7 0 1 0 6 4 2 3]\n",
            "  Predicted Labels (Batch): [1 0 0 6 0 0 6 0 2 0 1 0 0 2 2 1]\n",
            "  Matches: 5 / 16\n",
            "  Accuracy in batch: 0.3125\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -2.7969 / 2.0410\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 1 ---\n",
            "  True Labels (Batch): [0 1 3 0 1 4 2 7 4 2 2 7 0 0 1 1]\n",
            "  Predicted Labels (Batch): [1 1 6 1 1 6 5 2 4 6 4 6 1 1 1 6]\n",
            "  Matches: 4 / 16\n",
            "  Accuracy in batch: 0.2500\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -3.0938 / 2.7383\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 2 ---\n",
            "  True Labels (Batch): [3 3 7 5 5 3 1 4 3 7 2 4 4 4 2 7]\n",
            "  Predicted Labels (Batch): [1 6 0 1 6 1 1 6 1 6 1 6 4 6 0 0]\n",
            "  Matches: 2 / 16\n",
            "  Accuracy in batch: 0.1250\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -2.7246 / 2.6367\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 3 ---\n",
            "  True Labels (Batch): [1 5 7 3 5 4 4 2 0 1 7 7 3 7 1 0]\n",
            "  Predicted Labels (Batch): [1 0 0 1 5 7 4 1 1 1 7 7 7 7 1 0]\n",
            "  Matches: 9 / 16\n",
            "  Accuracy in batch: 0.5625\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -3.0742 / 2.0195\n",
            "------------------------------\n",
            "\n",
            "--- EVALUATION BATCH 4 ---\n",
            "  True Labels (Batch): [4 2 7 3 4 4 1 3 1 4 2 4 4 5 5 5]\n",
            "  Predicted Labels (Batch): [0 1 5 1 6 4 1 1 0 1 0 6 7 0 7 0]\n",
            "  Matches: 2 / 16\n",
            "  Accuracy in batch: 0.1250\n",
            "  Logits shape: torch.Size([16, 8]), Min/Max: -3.2930 / 3.2207\n",
            "------------------------------\n",
            "→ Validation accuracy after epoch 2: 0.3056\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/20:   0% 0/72 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5714191879d45fabfdddd35d9295f68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-f51e0e028915>:227: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-f51e0e028915>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-f51e0e028915>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"spec\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memotion_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-f51e0e028915>\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav_to_logmel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# [1, mels, T]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(waveform, orig_freq, new_freq, lowpass_filter_width, rolloff, resampling_method, beta)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m     )\n\u001b[0;32m-> 1530\u001b[0;31m     \u001b[0mresampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_sinc_resample_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36m_apply_sinc_resample_kernel\u001b[0;34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[0mnum_wavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m     \u001b[0mresampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m     \u001b[0mresampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_wavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m     \u001b[0mtarget_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_freq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0morig_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, numpy as np, pathlib, datetime as dt\n",
        "stats = {\"loss\": train_losses, \"val_acc\": val_accuracies}"
      ],
      "metadata": {
        "id": "uKzIkJHBxK9y"
      },
      "id": "uKzIkJHBxK9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, matplotlib.pyplot as plt, numpy as np, pathlib\n",
        "\n",
        "epochs = np.arange(1, len(train_losses) + 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = np.array(train_losses)\n",
        "window = 50\n",
        "\n",
        "# Compute the moving average (valid mode so we don’t pad)\n",
        "smoothed = np.convolve(loss, np.ones(window)/window, mode=\"valid\")\n",
        "\n",
        "# The x‐axis for the smoothed curve should start at “window/2” roughly\n",
        "iters = np.arange(len(smoothed)) + window//2\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iters, smoothed, label=f\"loss (smoothed, window={window})\")\n",
        "plt.xlabel(\"Training iteration\")\n",
        "plt.ylabel(\"Cross‐entropy loss\")\n",
        "plt.title(\"Smoothed training‐loss – Spectrogram CNN-Transformer\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_curve_smoothed.png\")\n",
        "plt.show()\n",
        "\n",
        "print(train_losses[-1])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "KCpvs_zHxLbg",
        "outputId": "3274734f-7063-4028-bb2f-fe75d9218343"
      },
      "id": "KCpvs_zHxLbg",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05948702494303385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "ckpt_dir = \"./exp_ravdess_spec_tr\"     # your HF dir\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(ckpt_dir)\n",
        "model.eval()\n",
        "\n",
        "# Save weights as a plain .pt state‑dict\n",
        "torch.save(model.state_dict(), \"best_w2v2.pt\")"
      ],
      "metadata": {
        "id": "-KodporP1Gme"
      },
      "id": "-KodporP1Gme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# fusion_ravdess.py\n",
        "\n",
        "\"\"\"\n",
        "Late-fusion ensemble for RAVDESS:\n",
        "    • Branch A: Wav2Vec2-base classifier\n",
        "    • Branch B: Spectrogram CNN-Transformer classifier\n",
        "    • LogisticRegression over softmax outputs (16-dim feature)\n",
        "    • Stratified validation split, softmax inputs, C=500 regularization\n",
        "    • MelSpectrogram parameters exactly match SpecCNNTr training:\n",
        "        – SR = 16 kHz, n_fft = int(16 000 × 0.025) = 400\n",
        "        – hop_length = int(16 000 × 0.010) = 160\n",
        "        – n_mels = 128\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "# ─── PATHS & CONFIGURATION ───────────────────────────────────────────────\n",
        "DATA_DIR   = path  # adjust to your environment\n",
        "W2V2_PT    = \"best_model.pt\"         # your fine-tuned Wav2Vec2 weights\n",
        "SPEC_PT    = \"best_cnn.pt\"         # your fine-tuned SpecCNNTr weights\n",
        "FUSION_OUT = \"./fusion_head2.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── LABEL UTILITY ───────────────────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in test_paths]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "print(f\"Training on {len(train_paths)} clips, validating on {len(val_paths)} clips.\")\n",
        "\n",
        "# ─── BRANCH A: Wav2Vec2-base ───────────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "\n",
        "state_w2v = torch.load(W2V2_PT, map_location=device)\n",
        "w2v2_model.load_state_dict(state_w2v)\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor @16 kHz, normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    inp = processor(wav.numpy(),\n",
        "                    sampling_rate=SR,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)  # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()         # [8]\n",
        "\n",
        "# ─── BRANCH B: Spectrogram CNN-Transformer ──────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "state_spec = torch.load(SPEC_PT, map_location=device)\n",
        "spec_model.load_state_dict(state_spec)\n",
        "spec_model.eval()\n",
        "\n",
        "# MelSpectrogram parameters exactly as used during SpecCNNTr training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 16 000 × 0.025 = 400\n",
        "    hop_length=int(SR * 0.010), # 16 000 × 0.010 = 160\n",
        "    n_mels=128                  # matches N_MELS during training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor [L], normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    # 1) Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1, 128, T']\n",
        "    spec_db = to_db(spec)               # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1, 1, 128, T']\n",
        "\n",
        "    # 2) Forward through the trained SpecCNNTr\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()      # [8]\n",
        "\n",
        "# ─── STEP 1: VERIFY EACH BRANCH ON VALIDATION SET ─────────────────────────\n",
        "def evaluate_branch(predict_fn, paths):\n",
        "    preds, gts = [], []\n",
        "    for p in paths:\n",
        "        wav, sr = torchaudio.load(p)\n",
        "        if sr != SR:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "        wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "        prob = predict_fn(wav)           # [8]\n",
        "        preds.append(int(prob.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "acc_w2v2 = evaluate_branch(probs_w2v2, val_paths)\n",
        "acc_spec = evaluate_branch(probs_spec, val_paths)\n",
        "print(f\"Wav2Vec2 branch accuracy on val:   {acc_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch accuracy on val:   {acc_spec:.4f}\")\n",
        "\n",
        "# ─── STEP 2: TRAIN LOGISTIC-REGRESSION FUSION ON SOFTMAX INPUTS ──────────\n",
        "X, y = [], []\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "    p1 = probs_w2v2(wav)           # [8]\n",
        "    p2 = probs_spec(wav)           # [8]\n",
        "    X.append(torch.cat([p1, p2]).numpy())  # 16-dim feature\n",
        "    y.append(emotion_id(str(p)))\n",
        "\n",
        "fusion = LogisticRegression(\n",
        "    C=500,                    # low L2 penalty → near-exact fit if possible\n",
        "    max_iter=2000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\"\n",
        ")\n",
        "fusion.fit(X, y)\n",
        "train_acc_lr = accuracy_score(y, fusion.predict(X))\n",
        "print(f\"LogisticRegression (train-on-val) acc = {train_acc_lr:.4f}\")\n",
        "\n",
        "with open(FUSION_OUT, \"wb\") as fout:\n",
        "    pickle.dump(fusion, fout)\n",
        "print(f\"Saved fusion head → {FUSION_OUT}\")\n",
        "\n",
        "# ─── STEP 3: SANITY CHECK ON A RANDOM VALIDATION CLIP ─────────────────────\n",
        "import numpy as np\n",
        "test_clip = random.choice(val_paths)\n",
        "wav, sr    = torchaudio.load(test_clip)\n",
        "if sr != SR:\n",
        "    wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "p1 = probs_w2v2(wav).numpy()\n",
        "p2 = probs_spec(wav).numpy()\n",
        "fused_probs = fusion.predict_proba([np.concatenate([p1, p2])])[0]\n",
        "predicted = int(fused_probs.argmax())\n",
        "true_label = emotion_id(str(test_clip))\n",
        "\n",
        "print(f\"Sanity check clip: {test_clip.name}\")\n",
        "print(f\" W2V2 predicts {int(p1.argmax())}, SpecCNNTr predicts {int(p2.argmax())}\")\n",
        "print(f\" Fusion predicts {predicted}, true = {true_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "ohCTqrcI3kwk",
        "outputId": "3e4725d2-f6bb-4fab-9fa1-f3c7dcc1ce58"
      },
      "id": "ohCTqrcI3kwk",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val label counts: Counter({1: 157, 5: 155, 2: 154, 3: 153, 7: 153, 4: 152, 6: 150, 0: 78})\n",
            "Training on 1152 clips, validating on 288 clips.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-f44ff4a34229>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0macc_w2v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_w2v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0macc_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Wav2Vec2 branch accuracy on val:   {acc_w2v2:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-f44ff4a34229>\u001b[0m in \u001b[0;36mevaluate_branch\u001b[0;34m(predict_fn, paths)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# [8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mgts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-f44ff4a34229>\u001b[0m in \u001b[0;36mprobs_w2v2\u001b[0;34m(wav)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     padding=True).to(device)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v2_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# [8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         \u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_weighted_layer_sum\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2254\u001b[0;31m         outputs = self.wav2vec2(\n\u001b[0m\u001b[1;32m   2255\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1726\u001b[0m         )\n\u001b[1;32m   1727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1728\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1729\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     )\n\u001b[1;32m   1028\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                     layer_outputs = layer(\n\u001b[0m\u001b[1;32m   1030\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mattn_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         hidden_states, attn_weights, _ = self.attention(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# get query proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;31m# get key, value proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;31m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# ─── IMPORT YOUR SPEC MODEL CLASS ─────────────────────────────────────────\n",
        "\n",
        "\n",
        "# ─── CONFIGURATION ────────────────────────────────────────────────────────\n",
        "DATA_DIR   = path # adjust\n",
        "W2V2_PT    = \"best_w2v2.pt\"\n",
        "SPEC_PT    = \"cnn_model.pt\"\n",
        "FUSION_PT  = \"fusion_head.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "ALPHA      = 0.5  # for weighted average\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── UTILITY: EXTRACT EMOTION ID ───────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "_, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# ─── LOAD BRANCH A: Wav2Vec2-base ─────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2_model.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD BRANCH B: SpecCNNTr ─────────────────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "# Use the same MelSpectrogram settings from training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 0.025 s at 16 kHz → n_fft = 400\n",
        "    hop_length=int(SR * 0.010), # 0.010 s at 16 kHz → hop_length = 160\n",
        "    n_mels=128                  # exactly as in SpecCNNTr’s training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1,128,T']\n",
        "    spec_db = to_db(spec)               # [1,128,T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1,1,128,T']\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD FUSION HEAD (Logistic Regression) ──────────────────────────────\n",
        "fusion = pickle.load(open(FUSION_PT, \"rb\"))\n",
        "\n",
        "# ─── EVALUATE ALL FOUR MODELS ─────────────────────────────────────────────\n",
        "y_true, preds_w2v2, preds_spec, preds_avg, preds_lr = [], [], [], [], []\n",
        "\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # normalize\n",
        "\n",
        "    # ground truth\n",
        "    true = emotion_id(str(p))\n",
        "    y_true.append(true)\n",
        "\n",
        "    # branch A: Wav2Vec2\n",
        "    p1 = probs_w2v2(wav).numpy()    # [8]\n",
        "    pred1 = int(np.argmax(p1))\n",
        "    preds_w2v2.append(pred1)\n",
        "\n",
        "    # branch B: SpecCNNTr\n",
        "    p2 = probs_spec(wav).numpy()    # [8]\n",
        "    pred2 = int(np.argmax(p2))\n",
        "    preds_spec.append(pred2)\n",
        "\n",
        "    # weighted average (alpha = 0.5)\n",
        "    fused_probs = ALPHA * p1 + (1 - ALPHA) * p2\n",
        "    preds_avg.append(int(np.argmax(fused_probs)))\n",
        "\n",
        "    # logistic regression fusion\n",
        "    fused_lr = fusion.predict([np.concatenate([p1, p2])])[0]\n",
        "    preds_lr.append(int(fused_lr))\n",
        "\n",
        "# ─── COMPUTE F1 SCORES (MACRO) ─────────────────────────────────────────────\n",
        "f1_w2v2 = f1_score(y_true, preds_w2v2, average=\"macro\")\n",
        "f1_spec = f1_score(y_true, preds_spec, average=\"macro\")\n",
        "f1_avg  = f1_score(y_true, preds_avg,  average=\"macro\")\n",
        "f1_lr   = f1_score(y_true, preds_lr,   average=\"macro\")\n",
        "# ─── VALIDATION LOSS CALCULATION ─────────────────────────────────────────────\n",
        "\n",
        "# We’ll accumulate total loss and then divide by number of samples:\n",
        "total_w2v2_loss = 0.0\n",
        "total_spec_loss = 0.0\n",
        "total_avg_loss  = 0.0\n",
        "total_lr_loss   = 0.0\n",
        "n = len(val_paths)\n",
        "\n",
        "for p in val_paths:\n",
        "    # Load & normalize waveform\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "\n",
        "    true_label = emotion_id(str(p))\n",
        "    label_tensor = torch.tensor([true_label]).long()  # shape [1]\n",
        "\n",
        "    # ---- Wav2Vec2 branch loss ----\n",
        "    # Get raw logits from Wav2Vec2 (before softmax)\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits_w2v = w2v2_model(**inp).logits.squeeze(0)  # [8] on device\n",
        "    loss_w2v2 = F.cross_entropy(logits_w2v.unsqueeze(0), label_tensor.to(device))\n",
        "    total_w2v2_loss += loss_w2v2.item()\n",
        "\n",
        "    # ---- SpecCNNTr branch loss ----\n",
        "    # Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))        # [1, 128, T']\n",
        "    spec_db = to_db(spec)                    # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)   # [1, 1, 128, T']\n",
        "    logits_spec = spec_model(spec_db).squeeze(0)  # [8] on device\n",
        "    loss_spec = F.cross_entropy(logits_spec.unsqueeze(0), label_tensor.to(device))\n",
        "    total_spec_loss += loss_spec.item()\n",
        "\n",
        "    # ---- Weighted-average ensemble loss ----\n",
        "    # We already have p1, p2 as probabilities, but we need fused_probs\n",
        "    with torch.no_grad():\n",
        "        # Recompute probabilities on CPU\n",
        "        p1 = F.softmax(logits_w2v, dim=-1).cpu().numpy()   # [8]\n",
        "        p2 = F.softmax(logits_spec, dim=-1).cpu().numpy()  # [8]\n",
        "        fused_probs = ALPHA * p1 + (1 - ALPHA) * p2         # [8] on CPU\n",
        "\n",
        "    # Cross-entropy for weighted average: -log(fused_probs[true_label])\n",
        "    eps = 1e-9  # avoid log(0)\n",
        "    total_avg_loss += -np.log(fused_probs[true_label] + eps)\n",
        "\n",
        "    # ---- Logistic Regression fusion loss ----\n",
        "    # Use fusion.predict_proba to get probabilities, then compute -log for true class\n",
        "    lr_probs = fusion.predict_proba([np.concatenate([p1, p2])])  # [8]\n",
        "    # print(lr_probs)\n",
        "    # total_lr_loss += -np.log(lr_probs[true_label] + eps)\n",
        "\n",
        "# Compute average loss over validation set\n",
        "val_loss_w2v2 = total_w2v2_loss / n\n",
        "val_loss_spec = total_spec_loss / n\n",
        "val_loss_avg  = total_avg_loss  / n\n",
        "# val_loss_lr   = total_lr_loss   / n\n",
        "\n",
        "# ─── PRINT VALIDATION LOSSES ─────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch   –  Val Loss = {val_loss_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch  –  Val Loss = {val_loss_spec:.4f}\")\n",
        "print(f\"Weighted Avg       –  Val Loss = {val_loss_avg:.4f}\")\n",
        "# print(f\"LR Fusion          –  Val Loss = {val_loss_lr:.4f}\")\n",
        "# ─── PRINT RESULTS ─────────────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch  –  Val Accuracy = {accuracy_score(y_true, preds_w2v2):.4f},  F1 (macro) = {f1_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch –  Val Accuracy = {accuracy_score(y_true, preds_spec):.4f},  F1 (macro) = {f1_spec:.4f}\")\n",
        "print(f\"Weighted Avg      –  Val Accuracy = {accuracy_score(y_true, preds_avg):.4f},  F1 (macro) = {f1_avg:.4f}\")\n",
        "print(f\"LR Fusion         –  Val Accuracy = {accuracy_score(y_true, preds_lr):.4f},  F1 (macro) = {f1_lr:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_ghUeP4zSRp",
        "outputId": "b2f5d3da-2f39-4c2f-a06d-06bce5f6ec82"
      },
      "id": "E_ghUeP4zSRp",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wav2Vec2 branch   –  Val Loss = 3.9374\n",
            "SpecCNNTr branch  –  Val Loss = 8.1557\n",
            "Weighted Avg       –  Val Loss = 4.4301\n",
            "Wav2Vec2 branch  –  Val Accuracy = 0.1042,  F1 (macro) = 0.0427\n",
            "SpecCNNTr branch –  Val Accuracy = 0.1250,  F1 (macro) = 0.0527\n",
            "Weighted Avg      –  Val Accuracy = 0.1181,  F1 (macro) = 0.0495\n",
            "LR Fusion         –  Val Accuracy = 0.1111,  F1 (macro) = 0.0467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Import your spectrogram model class and the wav→logmel helper\n",
        "\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────\n",
        "DATA_DIR      = path   # adjust\n",
        "W2V2_PT       = \"best_w2v2.pt\"\n",
        "SPEC_PT       = \"cnn_model.pt\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ── LABEL EXTRACTION ───────────────────────────────────────────\n",
        "import re\n",
        "EMO_RE = re.compile(r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "                    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1  # 0–7\n",
        "\n",
        "# ── COLLECT & SPLIT ───────────────────────────────────────────\n",
        "all_wavs = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_wavs,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=[emotion_id(str(p)) for p in all_wavs],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# ── LOAD Wav2Vec2 BRANCH ───────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_w2v2(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(\n",
        "        wav_tensor.numpy(),\n",
        "        sampling_rate=SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "    logits = w2v2(**inp).logits  # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)  # [8]\n",
        "\n",
        "# ── LOAD SPECTROGRAM BRANCH ───────────────────────────────────\n",
        "\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_spec(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    spec = wav_to_logmel(wav_tensor.unsqueeze(0))     # [1, n_mels, T]\n",
        "    spec = spec.unsqueeze(0).to(device)                # [1, 1, n_mels, T]\n",
        "    logits = spec_model(spec)                          # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)\n",
        "\n",
        "# ── EVALUATION UTILS ─────────────────────────────────────────\n",
        "def load_and_normalize(path: Path) -> torch.Tensor:\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "    return wav\n",
        "\n",
        "def evaluate_ensemble(alpha: float, val_files):\n",
        "    preds, gts = [], []\n",
        "    for p in val_files:\n",
        "        wav = load_and_normalize(p)\n",
        "        p1 = logits_w2v2(wav)   # [8]\n",
        "        p2 = logits_spec(wav)   # [8]\n",
        "        fused = alpha * p1 + (1.0 - alpha) * p2\n",
        "        preds.append(int(fused.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "# ── SEARCH BEST ALPHA ─────────────────────────────────────────\n",
        "best_alpha, best_acc = 0.0, 0.0\n",
        "for alpha in np.linspace(0.0, 1.0, 3):  # 0.0, 0.05, ..., 1.0\n",
        "    acc = evaluate_ensemble(alpha, val_paths)\n",
        "    if acc > best_acc:\n",
        "        best_acc, best_alpha = acc, alpha\n",
        "\n",
        "print(f\"Best weighted‐avg α = {best_alpha:.2f}, Val Acc = {best_acc:.4f}\")\n",
        "\n",
        "# ── OPTIONALLY: PRINT A FEW SAMPLE PREDICTIONS ─────────────────\n",
        "for i, p in enumerate(random.sample(val_paths, 5), 1):\n",
        "    wav = load_and_normalize(p)\n",
        "    p1 = logits_w2v2(wav).argmax().item()\n",
        "    p2 = logits_spec(wav).argmax().item()\n",
        "    fused = (best_alpha * logits_w2v2(wav) + (1.0 - best_alpha) * logits_spec(wav)).argmax().item()\n",
        "    true = emotion_id(str(p))\n",
        "    print(f\"{i}) {p.name} → W2V2={p1}, SPEC={p2}, FUSED={fused}, TRUE={true}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QMwZzs84ozg",
        "outputId": "05000d07-cb02-4d1f-8985-130b292f6a46"
      },
      "id": "8QMwZzs84ozg",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best weighted‐avg α = 0.50, Val Acc = 0.9115\n",
            "1) 03-01-08-01-01-01-17.wav → W2V2=0, SPEC=0, FUSED=0, TRUE=0\n",
            "2) 03-01-03-02-02-02-05.wav → W2V2=1, SPEC=1, FUSED=1, TRUE=1\n",
            "3) 03-01-08-02-01-02-11.wav → W2V2=1, SPEC=1, FUSED=1, TRUE=1\n",
            "4) 03-01-08-01-01-02-08.wav → W2V2=0, SPEC=0, FUSED=0, TRUE=0\n",
            "5) 03-01-07-02-01-02-10.wav → W2V2=1, SPEC=1, FUSED=1, TRUE=1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python-3.9.11",
      "language": "python",
      "name": "python-3.9.11"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71fa3b0f11bf4785ae1a3326f07783e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca50560a4ec04e0ca45fffa23cf22ddb",
              "IPY_MODEL_78346f06fb5544c785789517f5521d1e",
              "IPY_MODEL_f6dbb4c8e0cd4aaca81be647b79b01b5"
            ],
            "layout": "IPY_MODEL_6773e6823d244a1fa1fceea6aeeb5b20"
          }
        },
        "ca50560a4ec04e0ca45fffa23cf22ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6124a311024052ab90a2d9181cb075",
            "placeholder": "​",
            "style": "IPY_MODEL_25f74e6963384a59b117508a57930e8e",
            "value": ""
          }
        },
        "78346f06fb5544c785789517f5521d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f6417545f124023be3fb3af07612d2d",
            "max": 288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fd0893b6cb748c0b814f11b6d9d9cd9",
            "value": 288
          }
        },
        "f6dbb4c8e0cd4aaca81be647b79b01b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ec52e11c6a4d629af2946aa3b0b444",
            "placeholder": "​",
            "style": "IPY_MODEL_4501d091a54941d79c574a88fed80359",
            "value": "Epoch 1/3: 100% 288/288 [01:08&lt;00:00,  4.29it/s, loss=1.9053, lr=7.4e-5]"
          }
        },
        "6773e6823d244a1fa1fceea6aeeb5b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "ce6124a311024052ab90a2d9181cb075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f74e6963384a59b117508a57930e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f6417545f124023be3fb3af07612d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd0893b6cb748c0b814f11b6d9d9cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82ec52e11c6a4d629af2946aa3b0b444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4501d091a54941d79c574a88fed80359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac18347c55a4981b5ab1fb5fdae3287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_424395352d6548f9938c8dd1322bcd0f",
              "IPY_MODEL_30f1d6f5b7364d2381a39912a82ecd36",
              "IPY_MODEL_cb2a7a7fc56f4a8186c90c3d18e1b76b"
            ],
            "layout": "IPY_MODEL_03eaba4bd20a4b58bd0021c812e6d6de"
          }
        },
        "424395352d6548f9938c8dd1322bcd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a9b96f5274b4f6b806f6172d2c1109a",
            "placeholder": "​",
            "style": "IPY_MODEL_2568e99ff5284c0ca5462883f217373a",
            "value": ""
          }
        },
        "30f1d6f5b7364d2381a39912a82ecd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f321fa64bc144b4c931a70ea83babb94",
            "max": 288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65447255395e467c8a8825d6be011812",
            "value": 288
          }
        },
        "cb2a7a7fc56f4a8186c90c3d18e1b76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640d4bb42cdf4b4ab1fb649c435f6a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_89805eab08e54e878dfaa806215b48e9",
            "value": "Epoch 2/3: 100% 288/288 [01:07&lt;00:00,  4.15it/s, loss=0.5071, lr=3.7e-5]"
          }
        },
        "03eaba4bd20a4b58bd0021c812e6d6de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "4a9b96f5274b4f6b806f6172d2c1109a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2568e99ff5284c0ca5462883f217373a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f321fa64bc144b4c931a70ea83babb94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65447255395e467c8a8825d6be011812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "640d4bb42cdf4b4ab1fb649c435f6a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89805eab08e54e878dfaa806215b48e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d4e3a314514c1daf68d9a1e4939d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_388151add624408ca0f4d21fe59db0a8",
              "IPY_MODEL_d4019cbb45464a8abba16fd10f0f177d",
              "IPY_MODEL_a2789dae1eb24dbf900ad967ddcd5407"
            ],
            "layout": "IPY_MODEL_d98d219351a6415a8663fcfba8142db2"
          }
        },
        "388151add624408ca0f4d21fe59db0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2db9c68aee54f3ebbcd87483d9dda2c",
            "placeholder": "​",
            "style": "IPY_MODEL_75b7bf0245fb4b0d972cd60f08e73016",
            "value": ""
          }
        },
        "d4019cbb45464a8abba16fd10f0f177d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a0e4fa7edb463fad4e63246f0e3254",
            "max": 288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8a4d47d5d354566b3ccb8e8623b6e5f",
            "value": 288
          }
        },
        "a2789dae1eb24dbf900ad967ddcd5407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105188ef7f194f99b19fafb62e9f033d",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf0eb6b1b4a409181b435eaddfb280e",
            "value": "Epoch 3/3: 100% 288/288 [01:07&lt;00:00,  4.22it/s, loss=0.8330, lr=0]"
          }
        },
        "d98d219351a6415a8663fcfba8142db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "e2db9c68aee54f3ebbcd87483d9dda2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b7bf0245fb4b0d972cd60f08e73016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4a0e4fa7edb463fad4e63246f0e3254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a4d47d5d354566b3ccb8e8623b6e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "105188ef7f194f99b19fafb62e9f033d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf0eb6b1b4a409181b435eaddfb280e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47c99d92f7e94983b9fd2e10f34edea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ff150767ac849a8b82adf22738cba6a",
              "IPY_MODEL_2f5795d880344d09afb1b7a198b4c470",
              "IPY_MODEL_dcb563112df149f89e5808bf6aacca67"
            ],
            "layout": "IPY_MODEL_b46821753fa641bb98387fc3d25e83c5"
          }
        },
        "1ff150767ac849a8b82adf22738cba6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e6ec61a15144dc88fe3eaa6e2d3699",
            "placeholder": "​",
            "style": "IPY_MODEL_5f57df9f171f447c8ecaf3c6d370dccc",
            "value": ""
          }
        },
        "2f5795d880344d09afb1b7a198b4c470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a73c08b785e445db34b95f1d81f1322",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_308dba04dedc471d8ef052445ef0c91d",
            "value": 72
          }
        },
        "dcb563112df149f89e5808bf6aacca67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78b6cf434c84ebfbf077a17e924a559",
            "placeholder": "​",
            "style": "IPY_MODEL_5c77ed09d3c04620ba10d6d13a559709",
            "value": "Epoch 1/20: 100% 72/72 [00:30&lt;00:00,  2.37it/s, loss=1.9284, lr=2.65e-5]"
          }
        },
        "b46821753fa641bb98387fc3d25e83c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "27e6ec61a15144dc88fe3eaa6e2d3699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f57df9f171f447c8ecaf3c6d370dccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a73c08b785e445db34b95f1d81f1322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "308dba04dedc471d8ef052445ef0c91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f78b6cf434c84ebfbf077a17e924a559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c77ed09d3c04620ba10d6d13a559709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a395890c678644c5ad755fca1fa2c77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d94ccc0a44b4e85b61d34a0e60eb004",
              "IPY_MODEL_e689d798f2f44024ba87688c896d9136",
              "IPY_MODEL_eef5271917054746840f78334ac4093a"
            ],
            "layout": "IPY_MODEL_1c20318efe5447d8a1a19d88b240f184"
          }
        },
        "7d94ccc0a44b4e85b61d34a0e60eb004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f4c4b3217b41c9bba4a27808207dbf",
            "placeholder": "​",
            "style": "IPY_MODEL_ae54ca8ae96849689e4e9ca514294baf",
            "value": ""
          }
        },
        "e689d798f2f44024ba87688c896d9136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_548a278dc558456895148ef8827ec372",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25b1e8cfcc714355a9b792154bf45f3a",
            "value": 72
          }
        },
        "eef5271917054746840f78334ac4093a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b031e020c794e5884d9434971dbf7ed",
            "placeholder": "​",
            "style": "IPY_MODEL_b015973f7aa148fda7472ba114be1e87",
            "value": "Epoch 2/20: 100% 72/72 [00:29&lt;00:00,  2.55it/s, loss=1.9563, lr=5e-5]"
          }
        },
        "1c20318efe5447d8a1a19d88b240f184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "a1f4c4b3217b41c9bba4a27808207dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae54ca8ae96849689e4e9ca514294baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "548a278dc558456895148ef8827ec372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b1e8cfcc714355a9b792154bf45f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b031e020c794e5884d9434971dbf7ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b015973f7aa148fda7472ba114be1e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5714191879d45fabfdddd35d9295f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74184063fca14de291f5ca6d0eee3a3e",
              "IPY_MODEL_8f2d0f09db484add90f3ebadc78942a1",
              "IPY_MODEL_4465f5cb6df6452991752837565645d8"
            ],
            "layout": "IPY_MODEL_6061b6b092cc4af4a893e549b2f752d6"
          }
        },
        "74184063fca14de291f5ca6d0eee3a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8edd60c129bd4916a33c0c366762a3f9",
            "placeholder": "​",
            "style": "IPY_MODEL_87a1b486daa946c597ec07a7e566110f",
            "value": ""
          }
        },
        "8f2d0f09db484add90f3ebadc78942a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e73c6838a1544e69af50561bdb307640",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e49983c0a3a344328e27d275fea33195",
            "value": 6
          }
        },
        "4465f5cb6df6452991752837565645d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc423816599c4f8f91793e89f2da5496",
            "placeholder": "​",
            "style": "IPY_MODEL_786e2cde23774c3c86068f33f09854fe",
            "value": "Epoch 3/20:   8% 6/72 [00:02&lt;00:26,  2.45it/s, loss=1.7908, lr=5e-5]"
          }
        },
        "6061b6b092cc4af4a893e549b2f752d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "0"
          }
        },
        "8edd60c129bd4916a33c0c366762a3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a1b486daa946c597ec07a7e566110f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e73c6838a1544e69af50561bdb307640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e49983c0a3a344328e27d275fea33195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc423816599c4f8f91793e89f2da5496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786e2cde23774c3c86068f33f09854fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}