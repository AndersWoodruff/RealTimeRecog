{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4adade77",
      "metadata": {
        "id": "4adade77"
      },
      "source": [
        "### EMOVOICE: Real-time Speech Emotion Recognition Using Raw Audio Features and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02c13ea",
      "metadata": {
        "id": "b02c13ea"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "_wH9-oGOhk5D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wH9-oGOhk5D",
        "outputId": "ac8dfc11-a7ce-4ffc-ba46-af4fe7d273c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: noisereduce in /usr/local/lib/python3.11/dist-packages (3.0.3)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from noisereduce) (3.10.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: torch<2.7,>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub librosa numpy noisereduce evaluate transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "22f82f34",
      "metadata": {
        "id": "22f82f34"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torchaudio as ta\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch as t\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "from typing import Optional, Callable\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
        "outputId": "94eceb41-0ac7-4699-f51c-5e884de2ac54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "path = ('/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24')\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YFJ7pTHrxAzU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFJ7pTHrxAzU",
        "outputId": "d1a9803d-3fae-41f0-c492-59a0bd4a2891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Wav2Vec2‑base on RAVDESS with AMP, lr = 3e‑5,\n",
        "freeze encoder epoch 0, unfreeze top‑4 layers afterwards.\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path\n",
        "EXP_DIR    = \"./exp_ravdess_amp\"\n",
        "SR         = 16_000\n",
        "SEED       = 40\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "TRAIN_BS   = 4                   # GPU batch\n",
        "EVAL_BS    = 4\n",
        "GRAD_ACCUM = 2                   # 4×2 ⇒ eff 8\n",
        "LR         =  0.0001\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from dataclasses import dataclass\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------- DATASET ----------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(name: str) -> int:\n",
        "    return int(EMO_RE.search(name).group(3)) - 1          # 0‑7\n",
        "    # id = int(EMO_RE.search(name).group(3)) - 1\n",
        "    # if id == 0:\n",
        "    #     id = 1\n",
        "    # return id\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "    return wav.clamp_(-0.99, 0.99)\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def double_neutral(self):\n",
        "        for p in self.paths:\n",
        "            if emotion_id(p.name) == 0:\n",
        "                self.paths.append(p)\n",
        "        return self\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"wav\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "@dataclass\n",
        "class Collate:\n",
        "    def __call__(self, batch):\n",
        "        audio  = [b[\"wav\"].cpu().numpy() for b in batch]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        proc   = processor(audio, sampling_rate=SR,\n",
        "                           padding=True, return_tensors=\"pt\")\n",
        "        proc[\"labels\"] = labels\n",
        "        return proc\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p).double_neutral(), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p).double_neutral(),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "\n",
        "# ----------------------- MODEL ------------------------\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=8,\n",
        ").to(device)\n",
        "model.gradient_checkpointing_enable()\n",
        "model.freeze_feature_encoder()       # frozen for epoch 0\n",
        "\n",
        "# -------------------- OPT & SCHED ---------------------\n",
        "optimizer = Adam(model.parameters(), lr=LR,\n",
        "                  betas=(0.9,0.999), eps=1e-8)\n",
        "\n",
        "total_steps = (len(train_ld) // GRAD_ACCUM) * NUM_EPOCHS\n",
        "warm_steps  = int(0.1 * total_steps)\n",
        "scheduler   = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warm_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "# ---------------- ENABLE AMP --------------------------\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# -------------- TRAIN / VALIDATE ----------------------\n",
        "def evaluate():\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_ld:\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            preds.extend(logits.argmax(-1).cpu())\n",
        "            gts.extend(batch[\"labels\"].cpu())\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "step_global = 0\n",
        "train_loss_history   = []   # step‑level\n",
        "val_acc_history      = []   # epoch‑level\n",
        "best_acc             = 0.0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    # # --------- unfreeze top‑4 layers after epoch 0 ----------\n",
        "    # if epoch == 2:\n",
        "    #     for layer in model.wav2vec2.encoder.layer[-4:]:\n",
        "    #         for p in layer.parameters():\n",
        "    #             p.requires_grad = True\n",
        "    #     print(\"Unfroze top‑4 encoder layers.\")\n",
        "\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step_local, batch in enumerate(pbar, 1):\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = model(**batch).loss / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if step_local % GRAD_ACCUM == 0:\n",
        "            train_loss_history.append(loss.item() * GRAD_ACCUM)\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            step_global += 1\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    acc = evaluate()\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {acc:.4f}\")\n",
        "    val_acc_history.append(acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_model.pt\")\n",
        "\n",
        "print(f\"\\n*** Final accuracy after {NUM_EPOCHS} epochs: {acc:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rf9P24EI2ikw",
      "metadata": {
        "id": "Rf9P24EI2ikw"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"./exp_ravdess_amp/best_ckpt\"     # pick any folder name\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)             # ⇦ weights + config\n",
        "processor.save_pretrained(OUTPUT_DIR)         # ⇦ tokenizer / feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EslBPe8boMSc",
      "metadata": {
        "id": "EslBPe8boMSc"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spectrogram CNN‑Transformer for RAVDESS SER\n",
        "-------------------------------------------\n",
        "• Log‑mel spectrogram (128 bins, 25 ms / 10 ms)\n",
        "• 3‑layer 2‑D CNN → Transformer encoder (4 blocks, 8 heads)\n",
        "• Training: AMP, lr 3e‑5, batch 4 (+ grad‑accum 2) for 3 epochs\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR   = path  # <— set me\n",
        "EXP_DIR    = \"./exp_ravdess_spec_tr\"\n",
        "SR         = 16_000\n",
        "N_MELS         = 128\n",
        "NUM_EPOCHS     = 20         # ★\n",
        "LR_MAX         = 5e-5        # ★ peak of 1‑cycle\n",
        "LABEL_SMOOTH   = 0.1        # ★\n",
        "WIN_LEN    = 0.025        # 25 ms\n",
        "HOP_LEN    = 0.010        # 10 ms\n",
        "SEED       = 42\n",
        "TRAIN_BS   = 16\n",
        "EVAL_BS    = 16\n",
        "GRAD_ACCUM = 2\n",
        "\n",
        "# ----------------------- IMPORTS ----------------------\n",
        "import os, re, random, math\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.manual_seed(SEED); random.seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- DATASET -------------------------\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft= int(SR * WIN_LEN),\n",
        "    hop_length= int(SR * HOP_LEN),\n",
        "    n_mels=N_MELS,\n",
        ")\n",
        "db_transform = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "def wav_to_logmel(wav: torch.Tensor) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        spec = mel_spec(wav)        # [1, n_mels, T]\n",
        "        spec_db = db_transform(spec) # log‑mel\n",
        "    return spec_db                  # still [1, n_mels, T]\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True)     # mono, shape [1, L]\n",
        "    wav = wav / (wav.abs().max() + 1e-9)     # normalise\n",
        "    return wav_to_logmel(wav)               # [1, n_mels, T]\n",
        "\n",
        "class RAVDESS(Dataset):\n",
        "    def __init__(self, paths): self.paths = paths\n",
        "    def __len__(self): return len(self.paths)\n",
        "    def double_neutral(self):\n",
        "      for p in self.paths:\n",
        "          if emotion_id(p.name) == 0:\n",
        "              self.paths.append(p)\n",
        "      return self\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        return {\"spec\": load_audio(p), \"label\": emotion_id(p.name)}\n",
        "\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_p, val_p = train_test_split(\n",
        "    all_files, test_size=0.2,\n",
        "    stratify=[emotion_id(p.name) for p in all_files],\n",
        "    random_state=SEED,\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in val_p]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "\n",
        "# ------------------ PADDING COLLATE -------------------\n",
        "@dataclass\n",
        "class Collate:\n",
        "    pad_value: float = -80.0\n",
        "    def __call__(self, batch):\n",
        "        specs = [b[\"spec\"].squeeze(0) for b in batch]   # [n_mels, Ti]\n",
        "        labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "        max_T = max(s.shape[1] for s in specs)\n",
        "        padded = torch.stack([\n",
        "            torch.nn.functional.pad(s, (0, max_T - s.shape[1]), value=self.pad_value)\n",
        "            for s in specs\n",
        "        ])                                              # [B, n_mels, max_T]\n",
        "        return {\"spec\": padded.unsqueeze(1), \"labels\": labels}  # add channel dim\n",
        "\n",
        "train_ld = DataLoader(RAVDESS(train_p).double_neutral(), TRAIN_BS, True,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "val_ld   = DataLoader(RAVDESS(val_p).double_neutral(),   EVAL_BS, False,\n",
        "                      collate_fn=Collate(), pin_memory=True)\n",
        "print(val_ld)\n",
        "\n",
        "# --------------- CNN‑TRANSFORMER MODEL ----------------\n",
        "class SpecAug(nn.Module):\n",
        "    def __init__(self, freq_mask=15, time_mask=40, p=0.5):\n",
        "        super().__init__()\n",
        "        self.f = torchaudio.transforms.FrequencyMasking(freq_mask)\n",
        "        self.t = torchaudio.transforms.TimeMasking(time_mask)\n",
        "        self.p = p\n",
        "    def forward(self, x):\n",
        "        if self.training and torch.rand(1) < self.p:\n",
        "            x = self.f(x);  x = self.t(x)\n",
        "        return x\n",
        "\n",
        "spec_aug = SpecAug()\n",
        "\n",
        "def load_audio(path: Path):\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0, keepdim=True) / (wav.abs().max() + 1e-9)\n",
        "    spec = wav_to_logmel(wav)             # [1, mels, T]\n",
        "    return spec_aug(spec)                 # ★ augment on CPU\n",
        "\n",
        "# ---------------- CNN‑Transformer 2.0 -------- ★\n",
        "class SpecCNNTr(nn.Module):\n",
        "    def __init__(self, n_mels=N_MELS, n_classes=8):\n",
        "        super().__init__()\n",
        "        # CNN stem with SE blocks\n",
        "        self.cnn = nn.Sequential(\n",
        "            self._block(1, 64),   # out: [B,64,mel/2,T/2]\n",
        "            self._block(64,128),\n",
        "            self._block(128,192),\n",
        "            self._block(192,256),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "        d_model = 512\n",
        "        self.proj = nn.Linear(256, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=8,\n",
        "            dim_feedforward=1024, dropout=0.3, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.cls = nn.Linear(d_model, n_classes)\n",
        "    def _block(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "    def forward(self, x):                  # x [B,1,mels,T]\n",
        "        x = self.cnn(x)                    # [B,256,F',T']\n",
        "        x = self.pool(x).squeeze(2)        # [B,256,T']\n",
        "        x = self.proj(x.transpose(1,2))    # [B,T',512]\n",
        "        x = self.transformer(x)            # [B,T',512]\n",
        "        return self.cls(x.mean(1))\n",
        "\n",
        "model = SpecCNNTr().to(device)\n",
        "\n",
        "# --------------- optimiser & 1‑cycle ------------ ★\n",
        "optimizer = AdamW(model.parameters(), lr=LR_MAX,\n",
        "                  betas=(0.9,0.98), weight_decay=1e-2, eps=1e-8)\n",
        "steps_per_epoch = math.ceil(len(train_ld) / GRAD_ACCUM)\n",
        "total_steps     = steps_per_epoch * NUM_EPOCHS\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR_MAX,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1, anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
        "train_losses, val_accuracies = [], []\n",
        "train_loss_epoch = 0\n",
        "\n",
        "\n",
        "# ------------------- TRAIN / VAL ----------------------\n",
        "def evaluate(loader):\n",
        "    model.eval(); preds, gts = [], []\n",
        "    with torch.no_grad(), autocast():\n",
        "        for i, batch in enumerate(loader): # Added 'i' for indexing\n",
        "            spec = batch[\"spec\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device) # Get true labels for comparison\n",
        "\n",
        "            logits = model(spec)\n",
        "            predicted_classes = logits.argmax(-1) # This is a torch.Tensor of predicted class indices\n",
        "\n",
        "            # --- CRITICAL DEBUG FOR EVALUATION ---\n",
        "            if i < 5: # Only print for the first 5 batches in the validation set\n",
        "                print(f\"\\n--- EVALUATION BATCH {i} ---\")\n",
        "                print(f\"  True Labels (Batch): {labels.cpu().numpy()}\")\n",
        "                print(f\"  Predicted Labels (Batch): {predicted_classes.cpu().numpy()}\")\n",
        "\n",
        "                # Check for direct matches\n",
        "                matches = (labels == predicted_classes).cpu().numpy()\n",
        "                print(f\"  Matches: {matches.sum()} / {len(matches)}\")\n",
        "                print(f\"  Accuracy in batch: {matches.mean():.4f}\")\n",
        "\n",
        "                print(f\"  Logits shape: {logits.shape}, Min/Max: {logits.min().item():.4f} / {logits.max().item():.4f}\")\n",
        "                print(\"------------------------------\")\n",
        "            # -------------------------------------\n",
        "\n",
        "            preds.extend(predicted_classes.cpu().tolist()) # Convert to list for extend\n",
        "            gts.extend(labels.cpu().tolist()) # Convert to list for extend\n",
        "    return accuracy_score(gts, preds)\n",
        "scaler = GradScaler()\n",
        "best_acc = 0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_ld, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", ncols=0)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, batch in enumerate(pbar, 1):\n",
        "        spec = batch[\"spec\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            logits = model(spec)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels) / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        train_loss_epoch += loss.item() * GRAD_ACCUM\n",
        "        if step % GRAD_ACCUM == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            train_losses.append(train_loss_epoch / steps_per_epoch)\n",
        "            train_loss_epoch = 0\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*GRAD_ACCUM:.4f}\",\n",
        "                         lr=scheduler.get_last_lr()[0])\n",
        "\n",
        "    val_acc = evaluate(val_ld)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(),\"best_cnn.pt\")\n",
        "\n",
        "    print(f\"→ Validation accuracy after epoch {epoch}: {val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\n*** Training done. Final val acc: {val_acc:.4f} ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uKzIkJHBxK9y",
      "metadata": {
        "id": "uKzIkJHBxK9y"
      },
      "outputs": [],
      "source": [
        "import json, numpy as np, pathlib, datetime as dt\n",
        "stats = {\"loss\": train_losses, \"val_acc\": val_accuracies}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KCpvs_zHxLbg",
      "metadata": {
        "id": "KCpvs_zHxLbg"
      },
      "outputs": [],
      "source": [
        "import json, matplotlib.pyplot as plt, numpy as np, pathlib\n",
        "\n",
        "epochs = np.arange(1, len(train_losses) + 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = np.array(train_losses)\n",
        "window = 50\n",
        "\n",
        "# Compute the moving average (valid mode so we don’t pad)\n",
        "smoothed = np.convolve(loss, np.ones(window)/window, mode=\"valid\")\n",
        "\n",
        "# The x‐axis for the smoothed curve should start at “window/2” roughly\n",
        "iters = np.arange(len(smoothed)) + window//2\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iters, smoothed, label=f\"loss (smoothed, window={window})\")\n",
        "plt.xlabel(\"Training iteration\")\n",
        "plt.ylabel(\"Cross‐entropy loss\")\n",
        "plt.title(\"Smoothed training‐loss – Spectrogram CNN-Transformer\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_curve_smoothed.png\")\n",
        "plt.show()\n",
        "\n",
        "print(train_losses[-1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-KodporP1Gme",
      "metadata": {
        "id": "-KodporP1Gme"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "ckpt_dir = OUTPUT_DIR   # your HF dir\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(ckpt_dir)\n",
        "model.eval()\n",
        "\n",
        "# Save weights as a plain .pt state‑dict\n",
        "torch.save(model.state_dict(), \"best_w2v2.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ohCTqrcI3kwk",
      "metadata": {
        "id": "ohCTqrcI3kwk"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# fusion_ravdess.py\n",
        "\n",
        "\"\"\"\n",
        "Late-fusion ensemble for RAVDESS:\n",
        "    • Branch A: Wav2Vec2-base classifier\n",
        "    • Branch B: Spectrogram CNN-Transformer classifier\n",
        "    • LogisticRegression over softmax outputs (16-dim feature)\n",
        "    • Stratified validation split, softmax inputs, C=500 regularization\n",
        "    • MelSpectrogram parameters exactly match SpecCNNTr training:\n",
        "        – SR = 16 kHz, n_fft = int(16 000 × 0.025) = 400\n",
        "        – hop_length = int(16 000 × 0.010) = 160\n",
        "        – n_mels = 128\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "# ─── PATHS & CONFIGURATION ───────────────────────────────────────────────\n",
        "DATA_DIR   = path  # adjust to your environment\n",
        "W2V2_PT    = \"best_model.pt\"         # your fine-tuned Wav2Vec2 weights\n",
        "SPEC_PT    = \"best_cnn.pt\"         # your fine-tuned SpecCNNTr weights\n",
        "FUSION_OUT = \"./fusion_head2.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── LABEL UTILITY ───────────────────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "val_labels = [emotion_id(p.name) for p in val_paths]\n",
        "from collections import Counter\n",
        "print(\"Val label counts:\", Counter(val_labels))\n",
        "print(f\"Training on {len(train_paths)} clips, validating on {len(val_paths)} clips.\")\n",
        "\n",
        "# ─── BRANCH A: Wav2Vec2-base ───────────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "\n",
        "state_w2v = torch.load(W2V2_PT, map_location=device)\n",
        "w2v2_model.load_state_dict(state_w2v)\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor @16 kHz, normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    inp = processor(wav.numpy(),\n",
        "                    sampling_rate=SR,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)  # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()         # [8]\n",
        "\n",
        "# ─── BRANCH B: Spectrogram CNN-Transformer ──────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "state_spec = torch.load(SPEC_PT, map_location=device)\n",
        "spec_model.load_state_dict(state_spec)\n",
        "spec_model.eval()\n",
        "\n",
        "# MelSpectrogram parameters exactly as used during SpecCNNTr training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 16 000 × 0.025 = 400\n",
        "    hop_length=int(SR * 0.010), # 16 000 × 0.010 = 160\n",
        "    n_mels=128                  # matches N_MELS during training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    wav: 1-D torch.Tensor [L], normalized to [-1,1]\n",
        "    returns: 8-dim softmax probability on CPU\n",
        "    \"\"\"\n",
        "    # 1) Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1, 128, T']\n",
        "    spec_db = to_db(spec)               # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1, 1, 128, T']\n",
        "\n",
        "    # 2) Forward through the trained SpecCNNTr\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()      # [8]\n",
        "\n",
        "# ─── STEP 1: VERIFY EACH BRANCH ON VALIDATION SET ─────────────────────────\n",
        "def evaluate_branch(predict_fn, paths):\n",
        "    preds, gts = [], []\n",
        "    for p in paths:\n",
        "        wav, sr = torchaudio.load(p)\n",
        "        if sr != SR:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "        wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "        prob = predict_fn(wav)           # [8]\n",
        "        preds.append(int(prob.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "acc_w2v2 = evaluate_branch(probs_w2v2, val_paths)\n",
        "acc_spec = evaluate_branch(probs_spec, val_paths)\n",
        "print(f\"Wav2Vec2 branch accuracy on val:   {acc_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch accuracy on val:   {acc_spec:.4f}\")\n",
        "\n",
        "# ─── STEP 2: TRAIN LOGISTIC-REGRESSION FUSION ON SOFTMAX INPUTS ──────────\n",
        "X, y = [], []\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "    p1 = probs_w2v2(wav)           # [8]\n",
        "    p2 = probs_spec(wav)           # [8]\n",
        "    X.append(torch.cat([p1, p2]).numpy())  # 16-dim feature\n",
        "    y.append(emotion_id(str(p)))\n",
        "\n",
        "fusion = LogisticRegression(\n",
        "    C=500,                    # low L2 penalty → near-exact fit if possible\n",
        "    max_iter=2000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\"\n",
        ")\n",
        "fusion.fit(X, y)\n",
        "train_acc_lr = accuracy_score(y, fusion.predict(X))\n",
        "print(f\"LogisticRegression (train-on-val) acc = {train_acc_lr:.4f}\")\n",
        "\n",
        "with open(FUSION_OUT, \"wb\") as fout:\n",
        "    pickle.dump(fusion, fout)\n",
        "print(f\"Saved fusion head → {FUSION_OUT}\")\n",
        "\n",
        "# ─── STEP 3: SANITY CHECK ON A RANDOM VALIDATION CLIP ─────────────────────\n",
        "import numpy as np\n",
        "test_clip = random.choice(val_paths)\n",
        "wav, sr    = torchaudio.load(test_clip)\n",
        "if sr != SR:\n",
        "    wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)\n",
        "\n",
        "p1 = probs_w2v2(wav).numpy()\n",
        "p2 = probs_spec(wav).numpy()\n",
        "fused_probs = fusion.predict_proba([np.concatenate([p1, p2])])[0]\n",
        "predicted = int(fused_probs.argmax())\n",
        "true_label = emotion_id(str(test_clip))\n",
        "\n",
        "print(f\"Sanity check clip: {test_clip.name}\")\n",
        "print(f\" W2V2 predicts {int(p1.argmax())}, SpecCNNTr predicts {int(p2.argmax())}\")\n",
        "print(f\" Fusion predicts {predicted}, true = {true_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_ghUeP4zSRp",
      "metadata": {
        "id": "E_ghUeP4zSRp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# ─── IMPORT YOUR SPEC MODEL CLASS ─────────────────────────────────────────\n",
        "\n",
        "\n",
        "# ─── CONFIGURATION ────────────────────────────────────────────────────────\n",
        "DATA_DIR   = path # adjust\n",
        "folder     = r\"\"\n",
        "W2V2_PT    = folder + \"best_w2v2.pt\"\n",
        "SPEC_PT    = folder + \"best_cnn.pt\"\n",
        "FUSION_PT  = folder + \"fusion_head2.pkl\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.20\n",
        "ALPHA      = 0.5  # for weighted average\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ─── UTILITY: EXTRACT EMOTION ID ───────────────────────────────────────────\n",
        "EMO_RE = re.compile(\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\" # <--- This is the corrected regex\n",
        ")\n",
        "\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1\n",
        "\n",
        "# ─── COLLECT & STRATIFIED SPLIT ───────────────────────────────────────────\n",
        "all_files = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "labels    = [emotion_id(str(p)) for p in all_files]\n",
        "\n",
        "_, val_paths = train_test_split(\n",
        "    all_files,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=labels,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# ─── LOAD BRANCH A: Wav2Vec2-base ─────────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2_model.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_w2v2(wav: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits = w2v2_model(**inp).logits.squeeze(0)\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD BRANCH B: SpecCNNTr ─────────────────────────────────────────────\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "# Use the same MelSpectrogram settings from training:\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SR,\n",
        "    n_fft=int(SR * 0.025),      # 0.025 s at 16 kHz → n_fft = 400\n",
        "    hop_length=int(SR * 0.010), # 0.010 s at 16 kHz → hop_length = 160\n",
        "    n_mels=128                  # exactly as in SpecCNNTr’s training\n",
        ")\n",
        "to_db = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
        "\n",
        "@torch.no_grad()\n",
        "def probs_spec(wav: torch.Tensor) -> torch.Tensor:\n",
        "    spec = mel_spec(wav.unsqueeze(0))   # [1,128,T']\n",
        "    spec_db = to_db(spec)               # [1,128,T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)  # [1,1,128,T']\n",
        "    logits = spec_model(spec_db).squeeze(0)     # [8]\n",
        "    return F.softmax(logits, dim=-1).cpu()\n",
        "\n",
        "# ─── LOAD FUSION HEAD (Logistic Regression) ──────────────────────────────\n",
        "fusion = pickle.load(open(FUSION_PT, \"rb\"))\n",
        "\n",
        "# ─── EVALUATE ALL FOUR MODELS ─────────────────────────────────────────────\n",
        "y_true, preds_w2v2, preds_spec, preds_avg, preds_lr = [], [], [], [], []\n",
        "\n",
        "for p in val_paths:\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # normalize\n",
        "\n",
        "    # ground truth\n",
        "    true = emotion_id(str(p))\n",
        "    y_true.append(true)\n",
        "\n",
        "    # branch A: Wav2Vec2\n",
        "    p1 = probs_w2v2(wav).numpy()    # [8]\n",
        "    pred1 = int(np.argmax(p1))\n",
        "    preds_w2v2.append(pred1)\n",
        "\n",
        "    # branch B: SpecCNNTr\n",
        "    p2 = probs_spec(wav).numpy()    # [8]\n",
        "    pred2 = int(np.argmax(p2))\n",
        "    preds_spec.append(pred2)\n",
        "\n",
        "    # weighted average (alpha = 0.5)\n",
        "    fused_probs = ALPHA * p1 + (1 - ALPHA) * p2\n",
        "    preds_avg.append(int(np.argmax(fused_probs)))\n",
        "\n",
        "    # logistic regression fusion\n",
        "    fused_lr = fusion.predict([np.concatenate([p1, p2])])[0]\n",
        "    preds_lr.append(int(fused_lr))\n",
        "\n",
        "# ─── COMPUTE F1 SCORES (MACRO) ─────────────────────────────────────────────\n",
        "f1_w2v2 = f1_score(y_true, preds_w2v2, average=\"macro\")\n",
        "f1_spec = f1_score(y_true, preds_spec, average=\"macro\")\n",
        "f1_avg  = f1_score(y_true, preds_avg,  average=\"macro\")\n",
        "f1_lr   = f1_score(y_true, preds_lr,   average=\"macro\")\n",
        "# ─── VALIDATION LOSS CALCULATION ─────────────────────────────────────────────\n",
        "\n",
        "# We’ll accumulate total loss and then divide by number of samples:\n",
        "total_w2v2_loss = 0.0\n",
        "total_spec_loss = 0.0\n",
        "total_avg_loss  = 0.0\n",
        "total_lr_loss   = 0.0\n",
        "n = len(val_paths)\n",
        "\n",
        "for p in val_paths:\n",
        "    # Load & normalize waveform\n",
        "    wav, sr = torchaudio.load(p)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "\n",
        "    true_label = emotion_id(str(p))\n",
        "    label_tensor = torch.tensor([true_label]).long()  # shape [1]\n",
        "\n",
        "    # ---- Wav2Vec2 branch loss ----\n",
        "    # Get raw logits from Wav2Vec2 (before softmax)\n",
        "    inp = processor(wav.numpy(), sampling_rate=SR, return_tensors=\"pt\", padding=True).to(device)\n",
        "    logits_w2v = w2v2_model(**inp).logits.squeeze(0)  # [8] on device\n",
        "    loss_w2v2 = F.cross_entropy(logits_w2v.unsqueeze(0), label_tensor.to(device))\n",
        "    total_w2v2_loss += loss_w2v2.item()\n",
        "\n",
        "    # ---- SpecCNNTr branch loss ----\n",
        "    # Compute log-mel spectrogram exactly as in training\n",
        "    spec = mel_spec(wav.unsqueeze(0))        # [1, 128, T']\n",
        "    spec_db = to_db(spec)                    # [1, 128, T']\n",
        "    spec_db = spec_db.unsqueeze(0).to(device)   # [1, 1, 128, T']\n",
        "    logits_spec = spec_model(spec_db).squeeze(0)  # [8] on device\n",
        "    loss_spec = F.cross_entropy(logits_spec.unsqueeze(0), label_tensor.to(device))\n",
        "    total_spec_loss += loss_spec.item()\n",
        "\n",
        "    # ---- Weighted-average ensemble loss ----\n",
        "    # We already have p1, p2 as probabilities, but we need fused_probs\n",
        "    with torch.no_grad():\n",
        "        # Recompute probabilities on CPU\n",
        "        p1 = F.softmax(logits_w2v, dim=-1).cpu().numpy()   # [8]\n",
        "        p2 = F.softmax(logits_spec, dim=-1).cpu().numpy()  # [8]\n",
        "        fused_probs = ALPHA * p1 + (1 - ALPHA) * p2         # [8] on CPU\n",
        "\n",
        "    # Cross-entropy for weighted average: -log(fused_probs[true_label])\n",
        "    eps = 1e-9  # avoid log(0)\n",
        "    total_avg_loss += -np.log(fused_probs[true_label] + eps)\n",
        "\n",
        "    # ---- Logistic Regression fusion loss ----\n",
        "    # Use fusion.predict_proba to get probabilities, then compute -log for true class\n",
        "    lr_probs = fusion.predict_proba([np.concatenate([p1, p2])])  # [8]\n",
        "    # print(lr_probs)\n",
        "    # total_lr_loss += -np.log(lr_probs[true_label] + eps)\n",
        "\n",
        "# Compute average loss over validation set\n",
        "val_loss_w2v2 = total_w2v2_loss / n\n",
        "val_loss_spec = total_spec_loss / n\n",
        "val_loss_avg  = total_avg_loss  / n\n",
        "# val_loss_lr   = total_lr_loss   / n\n",
        "\n",
        "# ─── PRINT VALIDATION LOSSES ─────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch   –  Val Loss = {val_loss_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch  –  Val Loss = {val_loss_spec:.4f}\")\n",
        "print(f\"Weighted Avg       –  Val Loss = {val_loss_avg:.4f}\")\n",
        "# print(f\"LR Fusion          –  Val Loss = {val_loss_lr:.4f}\")\n",
        "# ─── PRINT RESULTS ─────────────────────────────────────────────────────────\n",
        "print(f\"Wav2Vec2 branch  –  Val Accuracy = {accuracy_score(y_true, preds_w2v2):.4f},  F1 (macro) = {f1_w2v2:.4f}\")\n",
        "print(f\"SpecCNNTr branch –  Val Accuracy = {accuracy_score(y_true, preds_spec):.4f},  F1 (macro) = {f1_spec:.4f}\")\n",
        "print(f\"Weighted Avg      –  Val Accuracy = {accuracy_score(y_true, preds_avg):.4f},  F1 (macro) = {f1_avg:.4f}\")\n",
        "print(f\"LR Fusion         –  Val Accuracy = {accuracy_score(y_true, preds_lr):.4f},  F1 (macro) = {f1_lr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8QMwZzs84ozg",
      "metadata": {
        "id": "8QMwZzs84ozg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Import your spectrogram model class and the wav→logmel helper\n",
        "\n",
        "\n",
        "# ─── CONFIG ────────────────────────────────────────────────\n",
        "DATA_DIR      = path   # adjust\n",
        "W2V2_PT       = \"best_w2v2.pt\"\n",
        "SPEC_PT       = \"best_cnn.pt\"\n",
        "SR         = 16_000\n",
        "SEED       = 42\n",
        "TEST_SIZE  = 0.2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ── LABEL EXTRACTION ───────────────────────────────────────────\n",
        "import re\n",
        "EMO_RE = re.compile(r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\-\"\n",
        "                    r\"([0-9]{2})\\-([0-9]{2})\\-([0-9]{2})\\.wav\")\n",
        "def emotion_id(fname: str) -> int:\n",
        "    return int(EMO_RE.search(Path(fname).name).group(3)) - 1  # 0–7\n",
        "\n",
        "# ── COLLECT & SPLIT ───────────────────────────────────────────\n",
        "all_wavs = sorted(Path(DATA_DIR).rglob(\"*.wav\"))\n",
        "train_paths, val_paths = train_test_split(\n",
        "    all_wavs,\n",
        "    test_size=TEST_SIZE,\n",
        "    stratify=[emotion_id(str(p)) for p in all_wavs],\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# ── LOAD Wav2Vec2 BRANCH ───────────────────────────────────────\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "w2v2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=8\n",
        ").to(device)\n",
        "w2v2.load_state_dict(torch.load(W2V2_PT, map_location=device))\n",
        "w2v2.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_w2v2(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    inp = processor(\n",
        "        wav_tensor.numpy(),\n",
        "        sampling_rate=SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "    logits = w2v2(**inp).logits  # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)  # [8]\n",
        "\n",
        "# ── LOAD SPECTROGRAM BRANCH ───────────────────────────────────\n",
        "\n",
        "spec_model = SpecCNNTr().to(device)\n",
        "spec_model.load_state_dict(torch.load(SPEC_PT, map_location=device))\n",
        "spec_model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def logits_spec(wav_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    spec = wav_to_logmel(wav_tensor.unsqueeze(0))     # [1, n_mels, T]\n",
        "    spec = spec.unsqueeze(0).to(device)                # [1, 1, n_mels, T]\n",
        "    logits = spec_model(spec)                          # [1, 8]\n",
        "    return torch.softmax(logits.squeeze(0).cpu(), dim=-1)\n",
        "\n",
        "# ── EVALUATION UTILS ─────────────────────────────────────────\n",
        "def load_and_normalize(path: Path) -> torch.Tensor:\n",
        "    wav, sr = torchaudio.load(path)\n",
        "    if sr != SR:\n",
        "        wav = torchaudio.functional.resample(wav, sr, SR)\n",
        "    wav = wav.mean(dim=0) / (wav.abs().max() + 1e-9)  # [L]\n",
        "    return wav\n",
        "\n",
        "def evaluate_ensemble(alpha: float, val_files):\n",
        "    preds, gts = [], []\n",
        "    for p in val_files:\n",
        "        wav = load_and_normalize(p)\n",
        "        p1 = logits_w2v2(wav)   # [8]\n",
        "        p2 = logits_spec(wav)   # [8]\n",
        "        fused = alpha * p1 + (1.0 - alpha) * p2\n",
        "        preds.append(int(fused.argmax()))\n",
        "        gts.append(emotion_id(str(p)))\n",
        "    return accuracy_score(gts, preds)\n",
        "\n",
        "# ── SEARCH BEST ALPHA ─────────────────────────────────────────\n",
        "best_alpha, best_acc = 0.0, 0.0\n",
        "for alpha in np.linspace(0.0, 1.0, 3):  # 0.0, 0.05, ..., 1.0\n",
        "    acc = evaluate_ensemble(alpha, val_paths)\n",
        "    if acc > best_acc:\n",
        "        best_acc, best_alpha = acc, alpha\n",
        "\n",
        "print(f\"Best weighted‐avg α = {best_alpha:.2f}, Val Acc = {best_acc:.4f}\")\n",
        "\n",
        "# ── OPTIONALLY: PRINT A FEW SAMPLE PREDICTIONS ─────────────────\n",
        "for i, p in enumerate(random.sample(val_paths, 5), 1):\n",
        "    wav = load_and_normalize(p)\n",
        "    p1 = logits_w2v2(wav).argmax().item()\n",
        "    p2 = logits_spec(wav).argmax().item()\n",
        "    fused = (best_alpha * logits_w2v2(wav) + (1.0 - best_alpha) * logits_spec(wav)).argmax().item()\n",
        "    true = emotion_id(str(p))\n",
        "    print(f\"{i}) {p.name} → W2V2={p1}, SPEC={p2}, FUSED={fused}, TRUE={true}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cnZU7WWBsoCH"
      },
      "id": "cnZU7WWBsoCH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}