{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adade77",
   "metadata": {},
   "source": [
    "### EMOVOICE: Real-time Speech Emotion Recognition Using Raw Audio Features and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c13ea",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub torchaudio librosa numpy torch noisereduce evaluate transformers[torch]\n",
    "# %pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f82f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import torchaudio as ta\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "from typing import Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1f1e0-a59e-4e85-a3a2-298bd28c5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b919e8c-d760-47ec-a763-0bac678997a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, segment_length=1000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.segment_length = segment_length        \n",
    "        \n",
    "        self.filelist = []\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for name in files:\n",
    "                self.filelist.append(os.path.join(root, name))\n",
    "        self.filelist = np.array(self.filelist)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if t.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        path = os.path.join(self.root_dir, self.filelist[idx])\n",
    "        \n",
    "        label = os.path.basename(path)\n",
    "        mod, chan, emo, inten, state, repit, act = str(label).split(\"-\")\n",
    "        \n",
    "        audio, rate = ta.load(path)\n",
    "        emotion = t.tensor(int(emo))\n",
    "\n",
    "\n",
    "        num_samples = int(self.segment_length * rate)\n",
    "        if audio.shape[1] > num_samples:\n",
    "            # Random crop for training\n",
    "            start = np.random.randint(0, audio.shape[1] - num_samples)\n",
    "            audio = audio[:, start:start+num_samples]\n",
    "        elif audio.shape[1] < num_samples:\n",
    "            # Pad with zeros if too short\n",
    "            padding = num_samples - audio.shape[1]\n",
    "            audio = t.nn.functional.pad(audio, (0, padding))\n",
    "\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "\n",
    "        return {\"input_values\": audio, \"label\": emotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "class Pipeline(nn.Module):\n",
    "    def __init__(self, input_rate, noise_reduce=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.noise_reduce = noise_reduce\n",
    "        self.input_rate = input_rate\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\", do_normalize=normalize)\n",
    "\n",
    "        if self.input_rate != self.feature_extractor.sampling_rate:\n",
    "            self.resampler = T.Resample(self.input_rate,  self.feature_extractor.sampling_rate)\n",
    "        else:\n",
    "            self.resampler = None\n",
    "\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        if self.resampler:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = t.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        if self.noise_reduce:\n",
    "            waveform_np = waveform.numpy()[0]\n",
    "            reduced_noise = nr.reduce_noise(\n",
    "                y=waveform_np, \n",
    "                sr=self.input_rate,\n",
    "                stationary=True\n",
    "            )\n",
    "            waveform = t.from_numpy(reduced_noise).unsqueeze(0)\n",
    "\n",
    "\n",
    "        waveform = self.feature_extractor(\n",
    "        waveform, sampling_rate=self.feature_extractor.sampling_rate, return_tensors=\"pt\")\n",
    "        \n",
    "        return waveform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = SpeechDataset(path, None)\n",
    "_, rate = ta.load(Data.filelist[0])\n",
    "\n",
    "pipe = Pipeline(rate)\n",
    "Data.transform = pipe\n",
    "trainset, testset = t.utils.data.random_split(Data, [0.85, 0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61af23b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
    "testloader = t.utils.data.DataLoader(testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10536cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_data\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cab6a",
   "metadata": {},
   "source": [
    "### Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=5):\n",
    "    \"\"\"Visualize random audio samples and their spectrograms\"\"\"\n",
    "    indices = np.random.choice(len(dataset), num_samples)\n",
    "    emotion_map = {\n",
    "        0: 'neutral', 1: 'calm', 2: 'happy', 3: 'sad',\n",
    "        4: 'angry', 5: 'fearful', 6: 'disgust', 7: 'surprised'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(15, 3*num_samples))\n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # Plot waveform\n",
    "        plt.subplot(num_samples, 2, 2*i+1)\n",
    "        plt.plot(sample['waveform'].squeeze().numpy())\n",
    "        plt.title(f\"Waveform - Emotion: {emotion_map[sample['emotion'].item()]}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        plt.subplot(num_samples, 2, 2*i+2)\n",
    "        plt.imshow(sample['spectrogram'].squeeze().numpy(), \n",
    "                  aspect='auto', origin='lower')\n",
    "        plt.title(\"Mel Spectrogram\")\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('Time frames')\n",
    "        plt.ylabel('Frequency bins')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff582ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_distribution(dataloader):\n",
    "    \"\"\"Plot histogram of emotion labels in the dataset\"\"\"\n",
    "    all_emotions = []\n",
    "    for batch in dataloader:\n",
    "        all_emotions.extend(batch['emotion'].tolist())\n",
    "    \n",
    "    emotion_map = {\n",
    "        0: 'neutral', 1: 'calm', 2: 'happy', 3: 'sad',\n",
    "        4: 'angry', 5: 'fearful', 6: 'disgust', 7: 'surprised'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x=all_emotions)\n",
    "    plt.xticks(ticks=range(8), labels=[emotion_map[i] for i in range(8)], rotation=45)\n",
    "    plt.title('Emotion Class Distribution')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "plot_emotion_distribution(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18342714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_augmentations(dataset, idx=0):\n",
    "    \"\"\"Compare original and augmented samples\"\"\"\n",
    "    original = dataset[idx]\n",
    "    augmented = dataset[idx]  # apply transform if defined\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Original waveform\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(original['waveform'].squeeze().numpy())\n",
    "    plt.title('Original Waveform')\n",
    "    \n",
    "    # Original spectrogram\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(original['spectrogram'].squeeze().numpy(), \n",
    "              aspect='auto', origin='lower')\n",
    "    plt.title('Original Spectrogram')\n",
    "    \n",
    "    # Augmented waveform\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(augmented['waveform'].squeeze().numpy())\n",
    "    plt.title('Augmented Waveform')\n",
    "    \n",
    "    # Augmented spectrogram\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(augmented['spectrogram'].squeeze().numpy(), \n",
    "              aspect='auto', origin='lower')\n",
    "    plt.title('Augmented Spectrogram')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# after add transforms\n",
    "\n",
    "# compare_augmentations(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e640176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_stats(dataloader):\n",
    "    \"\"\"Calculate mean and std of waveforms and spectrograms\"\"\"\n",
    "    waveform_sum = 0\n",
    "    waveform_sq_sum = 0\n",
    "    spectrogram_sum = 0\n",
    "    spectrogram_sq_sum = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        waveforms = batch['waveform']\n",
    "        spectrograms = batch['spectrogram']\n",
    "        \n",
    "        waveform_sum += waveforms.sum()\n",
    "        waveform_sq_sum += (waveforms**2).sum()\n",
    "        \n",
    "        spectrogram_sum += spectrograms.sum()\n",
    "        spectrogram_sq_sum += (spectrograms**2).sum()\n",
    "        \n",
    "        total_samples += waveforms.size(0)\n",
    "    \n",
    "    waveform_mean = waveform_sum / total_samples\n",
    "    waveform_std = (waveform_sq_sum / total_samples - waveform_mean**2)**0.5\n",
    "    \n",
    "    spectrogram_mean = spectrogram_sum / total_samples\n",
    "    spectrogram_std = (spectrogram_sq_sum / total_samples - spectrogram_mean**2)**0.5\n",
    "    \n",
    "    print(f\"Waveform - Mean: {waveform_mean:.4f}, Std: {waveform_std:.4f}\")\n",
    "    print(f\"Spectrogram - Mean: {spectrogram_mean:.4f}, Std: {spectrogram_std:.4f}\")\n",
    "    \n",
    "    return waveform_mean, waveform_std, spectrogram_mean, spectrogram_std\n",
    "\n",
    "wav_mean, wav_std, spec_mean, spec_std = calculate_batch_stats(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2db0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these values to use in your model\n",
    "stats = {\n",
    "    'waveform_mean': wav_mean,\n",
    "    'waveform_std': wav_std,\n",
    "    'spectrogram_mean': spec_mean,\n",
    "    'spectrogram_std': spec_std\n",
    "}\n",
    "\n",
    "# You can save them to a file\n",
    "# import pickle\n",
    "# with open('dataset_stats.pkl', 'wb') as f:\n",
    "#     pickle.dump(stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset[0]\n",
    "# waveform = sample['waveform']\n",
    "# print(\"Waveform stats:\")\n",
    "# print(f\"Min: {waveform.min().item():.4f}\")\n",
    "# print(f\"Max: {waveform.max().item():.4f}\")\n",
    "# print(f\"Mean: {waveform.mean().item():.4f}\")\n",
    "# print(f\"Std: {waveform.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac0310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
